{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "461f020c-153b-40cb-ac34-46ac3680cce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: anthropic in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (0.75.0)\n",
      "Requirement already satisfied: IPython in c:\\programdata\\anaconda3\\lib\\site-packages (8.27.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (0.17.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from IPython) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->IPython) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->IPython) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->IPython) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from asttokens->stack-data->IPython) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai anthropic IPython\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8fff397b-5feb-42e3-8f0f-1589026b3a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AI\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Checking keys\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "623d5c02-a33c-4aca-8e83-7b3ea2a13974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "Design a robust decision framework for selecting a single policy under extreme uncertainty in a partially observable environment with noisy and potentially adversarial data. Your answer should include:\n",
       "1) A model-agnostic decision rule or algorithm that is implementable with limited data and minimizes overfitting while providing interpretable rationale for the chosen policy.\n",
       "2) A clear set of assumptions, data requirements, and a plan to empirically test these assumptions using hypothetical but plausible data scenarios.\n",
       "3) At least three falsifiable predictions about outcomes under three distinct regimes (e.g., regime A: stable data-generating process with modest noise; regime B: non-stationarity and concept drift; regime C: adversarial manipulation).\n",
       "4) A monitoring and update protocol to detect distributional shift, data integrity issues, and biases, including trigger conditions, escalation paths, and rollback procedures.\n",
       "5) A thorough assessment of ethical, legal, and social implications for stakeholders, including potential biases and concrete mitigation strategies.\n",
       "6) A concise justification for why the approach is likely to perform well in practice, plus a discussion of key limitations and scenarios where it may fail.\n",
       "Please provide the final decision rule and justification in a structured, implementable format and avoid exposing step-by-step internal chain-of-thought.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for API call for question: **26.0937 seconds**\n"
     ]
    }
   ],
   "source": [
    "# Making a Question!\n",
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Make output in markdown format design\"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "# Setup question and display with markdown + timer\n",
    "openai = OpenAI()\n",
    "\n",
    "q_start_time = time.time() \n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "q_end_time = time.time()\n",
    "duration = q_end_time - q_start_time\n",
    "display(Markdown(question))\n",
    "print(f\"\\nTime taken for API call for question: **{duration:.4f} seconds**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb6c6cf5-8593-4fc4-907a-cd5a697f2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for the competition!\n",
    "\n",
    "competitors = []\n",
    "answers = []\n",
    "durations = []\n",
    "question += \"Don't make too long answer and arrange the output in Markdown format. No continuation of the chat, stop after the answer.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "daefe1b4-0d6a-420a-92a1-14ffe2938ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for gpt-5-mini API call: **63.0068 seconds**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Robust Decision Framework for Selecting a Single Policy under Extreme Uncertainty\n",
       "\n",
       "Summary: a model-agnostic, implementable rule (Robust Lower-bound Selection, RLS) that (a) evaluates candidate policies under an ensemble of plausible perturbations (noise, shifts, adversarial manipulations), (b) chooses the policy that maximizes a conservative performance metric (worst-case lower confidence bound or CVaR) while penalizing complexity, (c) produces an interpretable rationale and monitoring/rollback plan.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Final decision rule (algorithm: Robust Lower-bound Selection — RLS)\n",
       "\n",
       "Inputs\n",
       "- Candidate policy set P = {π1,...,πk} (can be rules, parametric policies, or black-box policies).\n",
       "- History dataset D = {(x_i, a_i, r_i, t_i, meta_i)} (covariates x, action a, reward/outcome r, timestamp t, metadata).\n",
       "- Baseline policy π0 (safe fallback).\n",
       "- Utility function U(π; D) (e.g., expected reward, harm-weighted utility).\n",
       "- Uncertainty generators G = {g1,...,gm} describing plausible perturbations (noise levels, covariate shifts, label corruption, adversarial fraction ε).\n",
       "- Off-policy evaluator E (IPS with clipped weights or Doubly Robust), bootstrap count B, confidence level α (e.g., 0.05), CVaR tail τ (optional, e.g., 0.1).\n",
       "- Complexity penalty C(π) (e.g., model size, interpretability cost), weight λ.\n",
       "- Trigger thresholds for monitoring (see §4).\n",
       "\n",
       "Outputs\n",
       "- Selected policy π* and interpretability report R (LCBs/CVaR across scenarios, sensitivity summary, failure modes).\n",
       "- Deployment plan & rollback steps.\n",
       "\n",
       "Procedure\n",
       "1. Data sanitization:\n",
       "   - Basic integrity checks (missingness, duplicates, signature/hashes), remove/flag corrupted rows.\n",
       "   - Estimate propensity scores p(a|x) if needed for IPS; apply clipping at w_max.\n",
       "\n",
       "2. Build perturbation scenarios:\n",
       "   - For each g ∈ G, define parameterized perturbations (e.g., additive noise σ, covariate shift magnitude δ, label flip rate η, fraction ε of adversarial examples).\n",
       "   - Use domain knowledge to bound ranges (conservative priors).\n",
       "\n",
       "3. Bootstrap evaluation:\n",
       "   - For b = 1..B:\n",
       "     - Resample D_b from D (with stratification if needed).\n",
       "     - For each scenario s generated from G:\n",
       "       - Apply perturbation s to D_b → D_{b,s}.\n",
       "       - For each candidate policy π:\n",
       "         - Compute off-policy estimate U_{b,s}(π) using E on D_{b,s}; record estimator variance.\n",
       "   - This yields distribution of U across (b,s) for each π.\n",
       "\n",
       "4. Compute robust score per π:\n",
       "   - Option A (Lower-confidence bound): LCB(π) = empirical α-quantile of U_{b,s}(π) across all (b,s).\n",
       "   - Option B (CVaR): CVaR_τ(π) = mean of worst τ fraction of U_{b,s}(π).\n",
       "   - RobustScore(π) = LCB(π) − λ·C(π) (or CVaR_τ(π) − λ·C(π)).\n",
       "   - Optionally compute worst-case regret relative to baseline: WorstRegret(π) = max_{b,s} (U_{b,s}(π0) − U_{b,s}(π)).\n",
       "\n",
       "5. Selection:\n",
       "   - Choose π* = argmax_π RobustScore(π), subject to constraint WorstRegret(π) ≤ R_max (set R_max per risk tolerance). If no π meets constraints, default to baseline π0.\n",
       "\n",
       "6. Rationale and interpretability output R:\n",
       "   - Report LCB/CVaR and median for chosen π*, complexity cost, most damaging perturbation scenarios, sensitivity charts, and policy decision rule summary.\n",
       "   - Provide simple explanation: e.g., \"π* selected because its 95% lower bound on expected utility under plausible perturbations is greatest and worst-case regret vs baseline ≤ X.\"\n",
       "\n",
       "7. Deploy with guardrails:\n",
       "   - Deploy π* in staged rollout (canary, A/B with heavy monitoring), see §4 for triggers and rollback.\n",
       "\n",
       "Implementation notes\n",
       "- Off-policy evaluation: use Doubly Robust or weighted IPS; clip importance weights and report sensitivity to clipping.\n",
       "- Keep candidate policies simple where possible (rules, small trees, linear policies) to improve interpretability and reduce overfitting.\n",
       "- Choose B (e.g., 500) and number of scenarios m pragmatically—enough for stable estimates given data size.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Assumptions, data requirements, and empirical test plan\n",
       "\n",
       "Assumptions (explicit)\n",
       "- A1: Utility function U correctly captures stakeholder objectives (or a defensible proxy exists).\n",
       "- A2: Candidate policies P contain at least one safe fallback π0.\n",
       "- A3: Historical data D is informative for the short-term behavior of the system (coverage of relevant covariate-action space).\n",
       "- A4: Perturbation family G contains realistic bounds for noise, shift, and adversary strength (conservative, but plausible).\n",
       "- A5: Off-policy evaluation bias is tolerable with clipping/DR corrections (no extreme propensities).\n",
       "\n",
       "Data requirements (minimum)\n",
       "- N_total: at least a few hundred independent episodes per key stratum; if scarce, rely more on conservative G and simpler policies.\n",
       "- Metadata: timestamps, provenance, and integrity checks.\n",
       "- Feature coverage: key covariates that influence action-outcome relationships must appear in D.\n",
       "- Baseline behavior data for π0 to compute regret.\n",
       "\n",
       "Plan to empirically test assumptions (hypothetical scenarios)\n",
       "- Create synthetic datasets that mimic domain features and labeled outcomes; vary noise, shift, adversarial fractions.\n",
       "- Tests:\n",
       "  - Coverage test (A3): remove certain covariate strata from training data; measure IPS variance and LCB widening.\n",
       "  - Sensitivity to G (A4): expand/shrink perturbation ranges; confirm selection stability (π* stable under reasonable changes).\n",
       "  - OPE validity (A5): compare off-policy estimates to on-policy returns in small controlled online rollouts (canary) to validate E calibration.\n",
       "- Acceptance criteria:\n",
       "  - If empirical LCBs consistently underestimate true on-policy canary returns by >δ, recalibrate E or tighten rolling deployment.\n",
       "  - If candidate selection flips across many plausible G changes, increase conservatism (larger λ or stricter WorstRegret).\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Falsifiable predictions (three regimes)\n",
       "\n",
       "Define metric M = realized average utility per episode measured on holdout/on-policy canary.\n",
       "\n",
       "Regime A — Stable, modest noise\n",
       "- Prediction A1: Within T_A (e.g., 2 weeks or 10k episodes), M(π*) ≥ median LCB(π*) + ε_A (ε_A small, e.g., 0.02 utility units).\n",
       "- Falsifiability: if M(π*) < LCB(π*) − δ_A, prediction fails.\n",
       "\n",
       "Regime B — Non-stationarity/concept drift\n",
       "- Prediction B1: M(π*) will degrade gradually; degradation slope s satisfies |ΔM/Δt| ≤ s_max over initial window W (we expect detect-and-update triggers before performance drops below baseline).\n",
       "- Falsifiability: if M(π*) drops abruptly below baseline π0 without preceding drift signals (distributional detectors not triggered), prediction fails.\n",
       "\n",
       "Regime C — Adversarial manipulation\n",
       "- Prediction C1: Under adversarial fraction ≤ ε_adversary (modeled in G), worst-case observed M(π*) ≥ WorstCase_LCB(π*) − ε_C (conservative), and monitoring will detect integrity anomalies (metadata tampering, abnormal reward patterns) within detection window T_C.\n",
       "- Falsifiability: if adversary-induced outcomes produce M(π*) significantly below WorstCase_LCB and no integrity flags are raised in T_C, prediction fails.\n",
       "\n",
       "Each prediction is falsifiable by comparing on-policy logged metrics to predicted intervals and by inspecting detector triggers.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Monitoring and update protocol (detection, escalation, rollback)\n",
       "\n",
       "Continuous monitoring components\n",
       "- Data-distribution monitors:\n",
       "  - Covariate shift: population stability index (PSI) and KL divergence on key features; threshold θ_cov (e.g., PSI > 0.2).\n",
       "  - Label/Outcome shift: compare recent outcome distribution to baseline with Earth Mover’s Distance; threshold θ_lab.\n",
       "- Model/performance monitors:\n",
       "  - Utility drift: rolling average of utility M with statistical control charts (CUSUM/Shewhart) at significance α_mon.\n",
       "  - Off-policy vs on-policy divergence: compare OPE predictions to canary actuals.\n",
       "- Integrity/anomaly monitors:\n",
       "  - Metadata validation (signatures, device IDs), outlier detection on timestamps and event rates.\n",
       "  - Adversarial detectors: sudden cluster of contradictory examples, reward spikes, or disproportionate error rates.\n",
       "- Fairness/bias monitors:\n",
       "  - Disparate impact (ratio metrics), subgroup utilities, intersectional checks; thresholds set by policy.\n",
       "\n",
       "Trigger conditions (examples)\n",
       "- Tier 1 (warning): PSI > θ1 or utility drift p-value < 0.05 → notify operations and data science.\n",
       "- Tier 2 (action required): PSI > θ2 or utility drop > X% vs baseline OR integrity anomaly → pause new exposures to π*; initiate canary rollback to π0 for affected segments.\n",
       "- Tier 3 (urgent rollback): evidence of data tampering or targeted harm (meet legal/ethical stop criteria) → immediate full rollback to π0 and incident response.\n",
       "\n",
       "Escalation path\n",
       "- Tier 1: Automated alert → data science on-call inspects dashboards within 4 hours.\n",
       "- Tier 2: Convene cross-functional response (ops, DS, legal, compliance) within 2 hours; restrict traffic to canary; collect forensic logs.\n",
       "- Tier 3: Immediate cessation; notify leadership and regulators as required; start post-incident review.\n",
       "\n",
       "Rollback & recovery procedures\n",
       "- Pre-deploy: ensure π0 is always deployable, versioned, and tested; keep feature transforms consistent.\n",
       "- Immediate rollback: switch routing to π0; preserve logs; mark affected data for quarantine.\n",
       "- Post-rollback: root-cause analysis, patch G and monitoring thresholds, retrain or replace candidate set if needed.\n",
       "- Audit trail: full logging of decision, metrics, time of rollback, and rationale; share with stakeholders.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Ethical, legal, and social implications & mitigation\n",
       "\n",
       "Key stakeholder impacts\n",
       "- Users potentially harmed by mistaken decisions (safety-critical domains).\n",
       "- Subpopulations disproportionately affected (bias).\n",
       "- Data subjects’ privacy and consent.\n",
       "- Organizational/legal exposure (regulation compliance).\n",
       "\n",
       "Risks\n",
       "- Distributional bias causing disproportionate harms.\n",
       "- Adversarial exploitation leading to targeted harms.\n",
       "- Over-reliance on automated decision with insufficient human oversight.\n",
       "- Privacy leaks from logs or models.\n",
       "\n",
       "Concrete mitigation strategies\n",
       "- Fairness constraints: enforce minimum subgroup utility floors during selection (add constraints to WorstRegret or per-group LCB≥g_min).\n",
       "- Transparency & explanation: produce R reporting interpretable rationale for chosen policy; publish summary for stakeholders.\n",
       "- Human-in-the-loop: require human approval for high-risk deployments and for Tier 2+ escalations.\n",
       "- Data governance: strict provenance, access controls, hashing/signatures for integrity, and retention policies.\n",
       "- Privacy protections: minimize sensitive attributes in decisioning; apply differential privacy or noise to analytics when needed.\n",
       "- Adversarial hardening: red-team tests, simulated adversary scenarios in G, input sanity checks.\n",
       "- Legal compliance: consult counsel for sector rules (e.g., GDPR, HIPAA); implement opt-out and data subject rights flows.\n",
       "- Independent audits: periodic external fairness/safety audits and public reporting where appropriate.\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Justification, expected performance, and limitations\n",
       "\n",
       "Why this will perform well in practice\n",
       "- Conservative selection (LCB/CVaR) reduces chance of worst-case catastrophic outcomes under uncertainty—suitable when stakes are high.\n",
       "- Model-agnostic: works with rules, simple models, or complex policies; off-policy evaluation avoids extensive online exploration.\n",
       "- Bootstrapping + scenario perturbations produce robust uncertainty quantification even with limited data.\n",
       "- Complexity penalty and preference for simpler policies reduce overfitting and improve interpretability.\n",
       "\n",
       "Key limitations and failure modes\n",
       "- Conservatism: may select suboptimal policy for benign environments (opportunity cost).\n",
       "- Unmodeled \"unknown unknowns\": if real-world perturbations lie outside G, selection may fail.\n",
       "- Dependence on OPE accuracy: heavy bias in E (poor propensities, extreme covariate mismatch) undermines estimates.\n",
       "- Candidate-policy coverage: if P lacks any robust good policy, selection cannot fix it—requires human design loop.\n",
       "- Data scarcity: with very small N, LCBs are wide and decision may default to baseline frequently.\n",
       "- Sophisticated adversary: if adversary can adapt in deployment beyond modeled capacity, monitoring might lag.\n",
       "\n",
       "Mitigations for limitations\n",
       "- Periodic expansion of G via adversarial red-teaming; incorporate rapid canary cycles to validate OPE.\n",
       "- Maintain policy development pipeline to generate new candidates when selection repeatedly returns π0.\n",
       "- Combine with conservative online learning only under strict safety constraints.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick parameter guidance (practical defaults)\n",
       "- B (bootstrap): 200–1000 depending on compute.\n",
       "- α for LCB: 0.05 (95% lower bound).\n",
       "- CVaR τ: 0.1 if risk-averse.\n",
       "- Importance weight clipping w_max: 10.\n",
       "- Complexity penalty λ: tuned so simple policy preferred unless complex policy improves LCB by >δ.\n",
       "- Monitoring windows: short-term (hourly) for integrity; medium-term (daily) for utility drift.\n",
       "\n",
       "---\n",
       "\n",
       "## Deliverables produced by the framework\n",
       "- Selected policy π* with decision report R (LCBs, worst-case scenarios, sensitivity).\n",
       "- Monitoring dashboard with PSI, utility charts, fairness metrics, integrity logs.\n",
       "- Escalation playbook and rollback scripts (automated routing to π0).\n",
       "- Test suite of simulated scenarios used to validate assumptions.\n",
       "\n",
       "---\n",
       "\n",
       "This framework is concise, implementable with limited data, and designed to minimize overfitting while producing interpretable justification for the chosen policy. It trades some short-term performance for robustness and safety under extreme uncertainty; use domain expertise to set G, thresholds, and the baseline policy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Competitor1\n",
    "\n",
    "model_name = \"gpt-5-mini\"\n",
    "\n",
    "c1_start_time = time.time()\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "c1_end_time = time.time()\n",
    "c1_duration = c1_end_time - c1_start_time\n",
    "\n",
    "print(f\"\\nTime taken for {model_name} API call: **{c1_duration:.4f} seconds**\")\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "durations.append(c1_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77f4dd68-cfd8-4aed-85cf-8c7e982f19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "# model_name = \"claude-sonnet-4-5\"\n",
    "\n",
    "# claude = Anthropic()\n",
    "# response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "# answer = response.content[0].text\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "\n",
    "# Deepseek\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "# model_name = \"deepseek-chat\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "841702dc-a678-4240-9cf8-fdea2112c14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for gemini-2.5-flash: **39.5234 seconds**\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The following robust decision framework is designed for selecting a single policy under extreme uncertainty, partial observability, and with noisy/adversarial data.\n",
       "\n",
       "---\n",
       "\n",
       "### **1. Model-Agnostic Decision Rule: Robust Scenario-Based Policy Selection (RSBPS)**\n",
       "\n",
       "The RSBPS framework prioritizes robustness and interpretability over predictive accuracy, which is highly challenging under extreme uncertainty and adversarial conditions. It identifies a policy that minimizes the maximum regret across a diverse set of plausible, stressful, and adversarial future scenarios.\n",
       "\n",
       "**Input:**\n",
       "*   A finite set of candidate policies: $P = \\{p_1, p_2, \\dots, p_k\\}$.\n",
       "*   A set of critical uncertainties and features derived from the partially observable, noisy data: $U = \\{u_1, u_2, \\dots, u_n\\}$. These represent key variables or processes that significantly impact policy outcomes but are subject to high uncertainty or manipulation.\n",
       "*   A performance metric: $M(p_i, s_j)$, a scalar value representing the outcome (e.g., cost, safety, efficiency, impact) of applying policy $p_i$ under scenario $s_j$. Assume $M$ is defined such that *lower values are better* (e.g., lower cost, fewer failures). If higher values are better, adjust regret calculation accordingly.\n",
       "\n",
       "**Algorithm:**\n",
       "\n",
       "1.  **Scenario Construction (S):**\n",
       "    *   **Define Plausible & Adversarial Scenarios:** Based on current and historical data, expert knowledge, and known adversarial tactics, construct a comprehensive set of future scenarios $S = \\{s_1, s_2, \\dots, s_m\\}$. Each $s_j$ describes a specific realization of the critical uncertainties $U$.\n",
       "    *   **Types of Scenarios:**\n",
       "        *   **Nominal:** Most likely or expected conditions (e.g., stable data-generating process with modest noise).\n",
       "        *   **Stress-Test:** Extreme but plausible conditions (e.g., significant but random data noise, unexpected market shifts, resource scarcity).\n",
       "        *   **Adversarial:** Conditions where data is actively manipulated or system behavior is targeted (e.g., data poisoning, spoofing, denial-of-service, concept drift induced by an adversary).\n",
       "        *   **Drift/Non-stationarity:** Scenarios explicitly modeling shifts in data distributions or underlying processes.\n",
       "        *   **Black Swan (if conceivable):** Highly improbable but high-impact events.\n",
       "    *   *Rationale:* This step integrates domain expertise and adversarial thinking directly into the decision process, bypassing the need for complex predictive models.\n",
       "\n",
       "2.  **Policy Performance Evaluation:**\n",
       "    *   For each policy $p_i \\in P$ and each scenario $s_j \\in S$, estimate or simulate the performance metric $M(p_i, s_j)$. This step may involve simplified models, expert elicitation, or historical data-driven simulations tailored to each scenario's conditions.\n",
       "    *   *Rationale:* Directly assesses policy outcomes under diverse conditions, rather than relying on a single, potentially flawed model of the environment.\n",
       "\n",
       "3.  **Regret Calculation:**\n",
       "    *   For each scenario $s_j$, identify the *optimal* performance among all candidate policies: $M_{optimal}(s_j) = \\min_{p_i \\in P} \\{M(p_i, s_j)\\}$.\n",
       "    *   Calculate the *scenario-specific regret* for each policy $p_i$ under scenario $s_j$: $R(p_i, s_j) = M(p_i, s_j) - M_{optimal}(s_j)$. This measures how much worse policy $p_i$ performs compared to the best possible policy *for that specific scenario*.\n",
       "    *   *Rationale:* Focuses on relative performance, which is more robust than absolute performance in highly uncertain environments.\n",
       "\n",
       "4.  **Max Regret Identification:**\n",
       "    *   For each policy $p_i$, identify its *maximum regret* across all scenarios: $MaxRegret(p_i) = \\max_{s_j \\in S} \\{R(p_i, s_j)\\}$. This represents the worst-case outcome for policy $p_i$ relative to the optimal choice for any given scenario.\n",
       "    *   *Rationale:* Explicitly identifies and quantifies the \"least worst\" performance, preparing for pessimistic outcomes.\n",
       "\n",
       "5.  **Policy Selection (Minimax Regret Criterion):**\n",
       "    *   Select the policy $p^*$ that minimizes the maximum regret:\n",
       "        $p^* = \\arg \\min_{p_i \\in P} \\{MaxRegret(p_i)\\}$\n",
       "    *   *Rationale:* This \"minimax regret\" strategy selects the policy that guarantees the best possible outcome under the worst possible scenario (relative to the optimal choice for that scenario). It's a robust choice when facing deep uncertainty and potential adversaries.\n",
       "\n",
       "**Interpretable Rationale:** The selected policy $p^*$ is chosen because it is the most robust option, minimizing the maximum potential \"missed opportunity\" or \"sub-optimality\" across all considered scenarios. Its rationale is directly tied to its comparative performance across the explicitly defined plausible, stressful, and adversarial futures.\n",
       "\n",
       "---\n",
       "\n",
       "### **2. Assumptions, Data Requirements, and Empirical Testing Plan**\n",
       "\n",
       "**Assumptions:**\n",
       "*   **Existence of Candidate Policies:** A finite, well-defined set of policies exists for consideration.\n",
       "*   **Feasibility of Scenario Definition:** It's possible to define a sufficiently rich and relevant set of scenarios (including adversarial ones) that capture the critical uncertainties.\n",
       "*   **Measurable Performance:** A quantifiable, agreed-upon performance metric $M$ can be defined for any policy under any scenario.\n",
       "*   **Bounded Rationality of Adversaries (for adversarial scenarios):** While adversaries are malicious, their capabilities and objectives are not entirely unknowable; there are limits to their manipulation.\n",
       "*   **Observable Indicators (even if noisy):** Despite partial observability and noise, some indicators or features (e.g., changes in correlation, distribution shifts, specific attack signatures) can be detected to inform scenario updates and monitoring.\n",
       "\n",
       "**Data Requirements:**\n",
       "*   **Historical Policy Performance Data:** Records of how past policies performed under varying conditions (if available).\n",
       "*   **Feature Data:** Current and historical data streams relevant to the critical uncertainties $U$, even if noisy or sparse. This informs scenario definition and monitoring.\n",
       "*   **Expert Elicitation:** Input from domain experts on plausible scenarios, potential adversarial tactics, and policy outcomes in specific situations.\n",
       "*   **Adversarial Intelligence:** Data on known attack vectors, adversarial capabilities, and objectives from security reports or threat intelligence.\n",
       "*   **Policy Specifications:** Detailed descriptions of each candidate policy $p_i$.\n",
       "\n",
       "**Empirical Testing Plan (Hypothetical Scenarios):**\n",
       "\n",
       "1.  **Regime A: Stable Data-Generating Process with Modest Noise**\n",
       "    *   **Hypothetical Setup:** Simulate a stable environment where underlying processes are constant, and data noise follows a well-behaved distribution (e.g., Gaussian). Candidate policies are designed for this environment.\n",
       "    *   **Test:**\n",
       "        *   Define scenarios including the nominal stable state and slight variations due to noise.\n",
       "        *   Run RSBPS to select a policy.\n",
       "        *   Generate multiple independent data runs from the stable process.\n",
       "        *   Evaluate the selected policy's performance.\n",
       "    *   **Expected Outcome:** RSBPS should consistently select a policy close to the empirically optimal policy for the stable regime, demonstrating its ability to find a robust choice even when uncertainty is minimal.\n",
       "\n",
       "2.  **Regime B: Non-stationarity and Concept Drift**\n",
       "    *   **Hypothetical Setup:** Introduce a gradual or abrupt shift in the data-generating process (e.g., changing correlations between features, altered underlying distributions) after an initial stable period.\n",
       "    *   **Test:**\n",
       "        *   Initially, use RSBPS under the stable regime.\n",
       "        *   Introduce the concept drift.\n",
       "        *   Use the monitoring protocol (Section 4) to detect the shift.\n",
       "        *   Upon detection, re-run RSBPS with *updated scenarios* that incorporate the detected drift.\n",
       "        *   Compare the performance of the *initial* selected policy versus the *re-selected* policy after drift detection.\n",
       "    *   **Expected Outcome:** The monitoring system should reliably detect the drift. RSBPS, when re-run with updated scenarios, should select a different, more appropriate policy that performs significantly better in the drifted environment than the initial policy.\n",
       "\n",
       "3.  **Regime C: Adversarial Manipulation**\n",
       "    *   **Hypothetical Setup:** Simulate an environment where a subset of input data is maliciously manipulated (e.g., data poisoning, feature injection) by an adversary aiming to degrade system performance or steer policy choice.\n",
       "    *   **Test:**\n",
       "        *   Define adversarial scenarios within RSBPS, explicitly modeling the adversary's capabilities and goals.\n",
       "        *   Run RSBPS.\n",
       "        *   Simulate the adversarial attacks on the data streams.\n",
       "        *   Evaluate the selected policy's performance under these attacks, and also evaluate other \"less robust\" policies (e.g., one selected by a purely predictive model without adversarial scenarios).\n",
       "    *   **Expected Outcome:** The policy selected by RSBPS should demonstrate greater resilience (i.e., less degradation in performance) against the simulated adversarial attacks compared to policies not chosen with adversarial scenarios in mind. Monitoring should detect integrity issues.\n",
       "\n",
       "---\n",
       "\n",
       "### **3. Falsifiable Predictions**\n",
       "\n",
       "1.  **Regime A (Stable Data-Generating Process with Modest Noise):**\n",
       "    *   **Prediction:** If the data-generating process is stable with modest noise, the RSBPS framework will select a policy whose average performance over 100 independent trials falls within 5% of the empirically optimal policy's average performance, and the selected policy will remain unchanged for at least 90% of re-evaluations (if no monitoring triggers).\n",
       "    *   **Falsification:** If the chosen policy's average performance is consistently more than 5% worse than the empirically optimal, or if the policy selection frequently changes without external triggers, the prediction is falsified.\n",
       "\n",
       "2.  **Regime B (Non-stationarity and Concept Drift):**\n",
       "    *   **Prediction:** When a significant concept drift (defined as a statistical distribution shift detectable by a Kolmogorov-Smirnov test at p<0.01 between pre- and post-drift data windows) occurs, the monitoring protocol will trigger a re-evaluation within 3 data cycles, leading to RSBPS selecting a *different* policy from the original one in at least 80% of drift events, and this new policy will perform at least 10% better than the original policy would have in the post-drift environment.\n",
       "    *   **Falsification:** If drift is not detected, or if the selected policy does not change, or if the new policy does not significantly outperform the old one post-drift, the prediction is falsified.\n",
       "\n",
       "3.  **Regime C (Adversarial Manipulation):**\n",
       "    *   **Prediction:** If 10% of input data is subjected to known adversarial poisoning attacks (e.g., label flipping, specific feature manipulation), the policy selected by RSBPS (which explicitly considers adversarial scenarios) will experience at most a 15% degradation in its performance metric, whereas a policy selected without considering adversarial scenarios would experience a degradation exceeding 30%. The monitoring system will also flag data integrity issues in over 95% of attack instances.\n",
       "    *   **Falsification:** If the RSBPS-selected policy degrades by more than 15%, or if the non-adversarially chosen policy degrades by less than 30%, or if the monitoring system fails to detect most attacks, the prediction is falsified.\n",
       "\n",
       "---\n",
       "\n",
       "### **4. Monitoring and Update Protocol**\n",
       "\n",
       "This protocol ensures the continued relevance and safety of the selected policy.\n",
       "\n",
       "**Key Components:**\n",
       "*   **Data Quality & Integrity Monitors:**\n",
       "    *   **Scope:** Raw input data, processed features, intermediary model outputs (if any).\n",
       "    *   **Checks:** Value range, completeness, consistency, statistical anomalies (e.g., sudden spikes/drops, outliers), feature correlations, entropy levels.\n",
       "    *   **Techniques:** Control charts (e.g., CUSUM, EWMA), statistical tests (e.g., Z-test for means, Chi-squared for distributions), anomaly detection algorithms (e.g., Isolation Forest, Autoencoders).\n",
       "*   **Distributional Shift Monitors:**\n",
       "    *   **Scope:** Key feature distributions, target variable distribution (if applicable), relationship between features and outcomes.\n",
       "    *   **Checks:** Change in mean, variance, skewness, kurtosis; shifts in multivariate distributions.\n",
       "    *   **Techniques:** Drift detection algorithms (e.g., ADWIN, DDM), statistical tests (e.g., Kolmogorov-Smirnov, Jensen-Shannon divergence, Adversarial validation), concept drift detectors.\n",
       "*   **Performance Monitors:**\n",
       "    *   **Scope:** Actual observed policy performance against the chosen metric $M$.\n",
       "    *   **Checks:** Deviations from expected performance, increasing variance, unexpected outcomes.\n",
       "    *   **Techniques:** A/B testing (if multiple policies are run in parallel in sub-segments), backtesting against historical data, control charts on performance metrics.\n",
       "*   **Bias Monitors:**\n",
       "    *   **Scope:** Disparate impact across defined demographic groups or critical sub-populations, fairness metrics (e.g., Equal Opportunity, Demographic Parity).\n",
       "    *   **Checks:** Discrepancies in performance $M$ or other impact metrics across groups.\n",
       "    *   **Techniques:** Regular auditing of group-specific outcomes.\n",
       "\n",
       "**Trigger Conditions:**\n",
       "*   **Severity 1 (Warning):**\n",
       "    *   Minor statistical anomalies or deviations (e.g., 1-sigma deviation in a control chart).\n",
       "    *   Small, localized shifts in feature distributions that don't yet impact overall performance.\n",
       "    *   Slight increase in uncertainty estimates.\n",
       "    *   *Action:* Automated alert to monitoring team, increased logging, deeper automated diagnostics.\n",
       "*   **Severity 2 (Alert):**\n",
       "    *   Persistent or significant statistical anomalies (e.g., 2-sigma deviation).\n",
       "    *   Detected distributional shift (e.g., K-S test p-value < 0.05).\n",
       "    *   Observed performance degradation (e.g., $M$ deviates by 5-10% from expected).\n",
       "    *   *Action:* Immediate alert to human operators/experts, detailed diagnostic report, initiation of a review process.\n",
       "*   **Severity 3 (Critical):**\n",
       "    *   Catastrophic data integrity issues (e.g., data source offline, massive corruption).\n",
       "    *   Major concept drift or non-stationarity (e.g., K-S test p-value < 0.01, ADWIN/DDM triggers).\n",
       "    *   Significant performance degradation (e.g., $M$ deviates by >10% from expected, or violates safety thresholds).\n",
       "    *   Detection of known adversarial attack signatures.\n",
       "    *   *Action:* Emergency protocol initiation, immediate policy review, potential temporary policy rollback.\n",
       "\n",
       "**Escalation Paths:**\n",
       "1.  **Automated Anomaly Detection (Severity 1):** System generates internal alerts and logs.\n",
       "2.  **Human Monitoring Team (Severity 2):** Designated team investigates alerts, performs root cause analysis, and determines if a policy re-evaluation is needed.\n",
       "3.  **Expert Panel/Stakeholder Review (Severity 3):** If critical issues arise, or a policy change is proposed, a panel of domain experts and key stakeholders (including ethical review board members) convenes to review diagnostics, assess new scenarios, and approve policy re-selection or emergency measures. This includes reviewing potential new adversarial scenarios.\n",
       "4.  **Policy Re-evaluation:** If drift or adversarial activity is confirmed, the RSBPS framework is re-run with updated scenarios reflecting the new reality, potentially leading to a new policy selection.\n",
       "\n",
       "**Rollback Procedures:**\n",
       "*   **Temporary Rollback:** In a critical situation, revert to a previously known \"safe\" or default policy. This policy should be pre-approved as a stable, albeit potentially sub-optimal, fallback.\n",
       "*   **Data Rollback/Sanitization:** If data integrity is compromised, attempt to quarantine or rollback to a clean data state, or apply specific sanitization techniques.\n",
       "*   **Version Control for Policies:** All policies and framework configurations are under version control, allowing easy reversion to previous stable versions.\n",
       "*   **Continuous Monitoring Post-Rollback:** Intensified monitoring of the rollback policy and data until confidence is restored and a new stable policy is selected.\n",
       "\n",
       "---\n",
       "\n",
       "### **5. Ethical, Legal, and Social Implications (ELSI)**\n",
       "\n",
       "**Stakeholders:**\n",
       "*   **Decision-makers:** Responsible for policy selection and outcomes.\n",
       "*   **Impacted Individuals/Groups:** Those directly affected by the policy's implementation (e.g., customers, citizens, employees).\n",
       "*   **Adversaries:** Entities attempting to manipulate the system (indirectly affected by countermeasures).\n",
       "*   **Regulators/Oversight Bodies:** Enforcing legal and ethical guidelines.\n",
       "*   **Developers/Operators:** Building and maintaining the system.\n",
       "\n",
       "**Potential Biases:**\n",
       "*   **Scenario Definition Bias:** Scenarios may inadvertently overlook critical edge cases or vulnerabilities, especially for marginalized groups, or be biased by the limited imagination/experience of experts. Adversarial scenarios might over-emphasize known threats and miss novel ones.\n",
       "*   **Performance Metric Bias:** The chosen metric $M$ might implicitly favor certain outcomes or groups over others, failing to capture holistic impacts or being susceptible to \"goodhart's law\" (when a measure becomes a target, it ceases to be a good measure).\n",
       "*   **Data Collection Bias:** Historical data used for scenario calibration or monitoring might reflect existing societal biases, leading to policies that perpetuate inequities. Partially observable data exacerbates this as critical variables might be missing or skewed.\n",
       "*   **Expert Elicitation Bias:** Experts' judgments can be influenced by personal biases, overconfidence, or limited perspectives.\n",
       "*   **Rollback Bias:** Default/fallback policies might not be equally fair or effective for all groups, potentially exacerbating existing biases during crisis.\n",
       "\n",
       "**Concrete Mitigation Strategies:**\n",
       "\n",
       "1.  **Diverse Scenario Teams & Red Teaming:**\n",
       "    *   **Strategy:** Form scenario construction teams with diverse backgrounds, expertise, and perspectives. Actively involve \"red teams\" (including those with adversarial mindsets or from marginalized groups) to identify overlooked vulnerabilities, biases, and adversarial attack vectors. Conduct \"pre-mortems\" to identify ways the system could fail.\n",
       "    *   **Mitigation:** Reduces scenario definition bias and enhances the robustness against novel adversarial threats and ethical blind spots.\n",
       "\n",
       "2.  **Multi-Objective & Equity-Aware Performance Metrics:**\n",
       "    *   **Strategy:** Define performance using a set of weighted metrics, explicitly including fairness metrics (e.g., disparate impact, equal opportunity) alongside operational metrics. Ensure transparent justification for weighting. Conduct regular audits of these metrics across different demographic or critical sub-groups.\n",
       "    *   **Mitigation:** Addresses performance metric bias and ensures equitable outcomes are considered during policy selection.\n",
       "\n",
       "3.  **Data Audits and Bias Detection Tools:**\n",
       "    *   **Strategy:** Implement automated and manual processes to audit data sources for representational biases, historical inequities, and adversarial tampering. Use fairness-aware AI tools to detect and quantify biases in data distributions. Augment limited data with synthetic, debiased data or transfer learning from more diverse datasets, when appropriate and safe.\n",
       "    *   **Mitigation:** Identifies and potentially corrects data collection bias, making the information informing scenarios more reliable and equitable.\n",
       "\n",
       "4.  **Transparent Decision Records & Justification:**\n",
       "    *   **Strategy:** Maintain a clear, human-readable log of all scenarios considered, their assessed impacts, and the explicit rationale for the chosen policy ($p^*$ minimizes MaxRegret across $S$). This includes documentation of expert inputs and their justifications.\n",
       "    *   **Mitigation:** Provides interpretability and accountability, allowing stakeholders to understand *why* a particular policy was chosen, facilitating ethical review and legal compliance.\n",
       "\n",
       "5.  **Stakeholder Engagement & Ethical Review Board:**\n",
       "    *   **Strategy:** Establish an independent ethical review board (ERB) composed of ethicists, legal experts, community representatives, and technical experts. This board reviews scenarios, performance metrics, and policy selections, especially during critical escalations. Engage affected communities in understanding policy impacts.\n",
       "    *   **Mitigation:** Ensures broad ethical, legal, and social considerations are integrated into the framework, providing external oversight and legitimacy.\n",
       "\n",
       "---\n",
       "\n",
       "### **6. Justification, Limitations, and Failure Scenarios**\n",
       "\n",
       "**Justification for Practical Performance:**\n",
       "\n",
       "The RSBPS framework is likely to perform well in the specified context due to its inherent design principles:\n",
       "1.  **Robustness to Uncertainty:** By focusing on minimax regret across a wide range of scenarios (including extreme and adversarial ones), it explicitly guards against worst-case outcomes rather than hoping for best-case predictions. This is crucial when uncertainty is extreme, and predictions are unreliable.\n",
       "2.  **Interpretability:** The rationale for selecting a policy is derived directly from its performance across well-defined scenarios. This transparency allows stakeholders to understand the trade-offs and risks, fostering trust and facilitating human oversight and intervention.\n",
       "3.  **Resilience to Adversarial Data:** Incorporating adversarial scenarios directly into the decision-making process forces the selection of policies that are inherently more resilient to manipulation, rather than attempting to filter out all adversarial data (which is often an impossible task).\n",
       "4.  **Limited Data & Model Agnosticism:** It does not require vast amounts of historical data for complex model training. Instead, it leverages available data to inform scenario definition, and expert knowledge to assess policy outcomes within those scenarios. This avoids overfitting to sparse or noisy data.\n",
       "5.  **Built-in Monitoring & Adaptability:** The comprehensive monitoring protocol, combined with explicit re-evaluation triggers and rollback procedures, ensures that the system can detect changes in the environment (including adversarial attacks or concept drift) and adapt the policy accordingly, preventing prolonged periods of sub-optimal or harmful operation.\n",
       "\n",
       "**Key Limitations:**\n",
       "\n",
       "1.  **Scenario Completeness:** The framework's effectiveness heavily relies on the completeness and relevance of the defined scenarios. Overlooking critical scenarios or underestimating adversarial capabilities can lead to selecting a sub-optimal or vulnerable policy.\n",
       "2.  **Expert Elicitation Burden & Bias:** Defining scenarios and evaluating policy outcomes within them requires significant input from domain experts. This can be time-consuming, costly, and susceptible to expert biases or limited imagination.\n",
       "3.  **Computational Complexity (for large P or S):** If the number of candidate policies ($k$) or scenarios ($m$) is very large, the evaluation step can become computationally intensive, especially if complex simulations are required for each $M(p_i, s_j)$.\n",
       "4.  **Metric Definition Challenge:** Defining a universally agreed-upon and unbiased performance metric $M$ can be challenging, especially when dealing with complex socio-technical systems.\n",
       "5.  **No Guarantee of Optimal Performance:** Minimax regret is a pessimistic strategy; it aims to minimize the *maximum possible loss*, not to achieve the *best possible average outcome*. In environments that turn out to be consistently stable and benign, a more optimistic approach might yield better average performance.\n",
       "\n",
       "**Scenarios Where It May Fail:**\n",
       "\n",
       "1.  **Unforeseeable Black Swans:** If a truly novel and impactful \"black swan\" event occurs that was entirely outside the scope of *any* conceivable scenario (even adversarial ones), the selected policy may be ill-equipped, and the monitoring system might not immediately identify it.\n",
       "2.  **Incapacitated Experts/Lack of Domain Knowledge:** If there are no reliable domain experts available to help define scenarios and assess policy outcomes, or if their knowledge base is fundamentally flawed or outdated, the framework cannot be effectively initialized or maintained.\n",
       "3.  **Complete Observability Breakdown:** If the environment becomes completely unobservable (e.g., all data streams fail, are utterly corrupted, or are entirely under adversarial control), monitoring becomes impossible, and the framework loses its ability to react or adapt.\n",
       "4.  **Sophisticated, Undetectable Adversary:** An adversary with unlimited resources and novel, zero-day attack capabilities that completely bypass all known detection mechanisms and were not envisioned in any adversarial scenario could consistently degrade performance without triggering a response.\n",
       "5.  **Decision Paralysis from Too Many Scenarios:** If the number of scenarios or policies becomes unmanageably large, or if consensus cannot be reached on scenario relevance or performance metrics, the framework could become too slow or complex to implement effectively.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Competitor 2\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "c2_start_time = time.time()\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "c2_end_time = time.time()\n",
    "c2_duration = c2_end_time - c2_start_time\n",
    "\n",
    "print(f\"Time taken for {model_name}: **{c2_duration:.4f} seconds**\\n\")\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "durations.append(c2_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d4e93fd-fdf9-49d1-a476-17a741b6542f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for openai/gpt-oss-120b: **6.0449 seconds**\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 1. Model‑agnostic Decision Rule  \n",
       "**Robust Policy‑Selection (RPS)** – choose the policy whose *worst‑case* expected utility, estimated from the data, is highest.  \n",
       "\n",
       "| Step | Action | Rationale |\n",
       "|------|--------|-----------|\n",
       "| 1️⃣| **Define a finite policy set** 𝒫 = {π₁,…,π_K}. | Keeps the problem tractable and interpretable. |\n",
       "| 2️⃣| **Collect a limited‑size batch** D = { (o_t, a_t, r_t) } where o_t are observable cues, a_t the action taken, r_t the noisy reward. | Works with sparse data; no need for full model. |\n",
       "| 3️⃣| **Bootstrap‑aggregate utility estimates**: for each π_k, draw B ≤ 200 bootstrap resamples of D, compute the average reward ˆU_k^b on each resample, and retain the **α‑lower‑confidence bound** L_k = 𝑄_{α}( {ˆU_k^b} ), e.g. α = 0.05. | Guarantees a pessimistic estimate that protects against over‑fitting and noise. |\n",
       "| 4️⃣| **Apply a simple regularizer**: L_k ← L_k – λ·|π_k| (policy complexity, e.g. number of decision rules). | Favors parsimonious policies → interpretability. |\n",
       "| 5️⃣| **Select** 𝜋* = arg max_{π_k∈𝒫} L_k. | Maximizes the worst‑case plausible performance. |\n",
       "| 6️⃣| **Provide rationale**: report L_k, bootstrap distribution, and complexity penalty for the chosen π*. | Transparent justification for stakeholders. |\n",
       "\n",
       "*Implementation notes* – The algorithm needs only reward samples, no model fitting. Bootstrapping is O(K·B·|D|) and can be run on a single CPU.  \n",
       "\n",
       "---\n",
       "\n",
       "## 2. Assumptions, Data Requirements & Empirical Test Plan  \n",
       "\n",
       "| Assumption | What it means | Data needed | Test (hypothetical) |\n",
       "|------------|---------------|------------|----------------------|\n",
       "| **A1 – Partial observability** | We only see a noisy proxy o_t of the true state. | Observations o_t, actions a_t, rewards r_t. | Simulate a hidden Markov process; compare estimated L_k with ground‑truth utility. |\n",
       "| **A2 – Bounded noise** | Reward noise is sub‑Gaussian with known variance proxy σ². | Repeated reward measurements for same (o,a). | Inject Gaussian noise of variance σ² into synthetic rewards; verify coverage of bootstrap L_k (≈95%). |\n",
       "| **A3 – Limited adversarial contamination** | ≤ ε % of data may be arbitrarily corrupted. | Flagged or suspect records (optional). | Replace ε % of rewards with extreme outliers; check that selected policy remains unchanged if λ is sufficiently large. |\n",
       "| **A4 – Finite policy set captures near‑optimal actions** | True optimal policy is approximated by some π_k ∈ 𝒫. | Policy library constructed a‑priori (e.g., decision trees of depth ≤ 3). | In simulated environments, generate optimal policy and verify it lies within distance ε of a library member. |\n",
       "\n",
       "*Empirical plan*: For each regime (A–C) generate 10 synthetic datasets (|D|=200) and run the RPS pipeline. Record (i) selection stability, (ii) coverage of true utility by L_k, and (iii) false‑positive rate of adversarial detection.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Falsifiable Predictions  \n",
       "\n",
       "| Regime | Prediction (testable) |\n",
       "|--------|-----------------------|\n",
       "| **A – Stable DG with modest noise** | *P1*: The selected policy’s true expected reward will be within 5 % of the oracle optimal reward in ≥ 80 % of simulations. |\n",
       "| **B – Non‑stationarity / concept drift** | *P2*: After a drift point, the bootstrap‑based L_k will drop by > δ (e.g., 0.2) for the previously optimal policy, triggering a policy change in ≤ 5 % of runs (i.e., low false‑alarm). |\n",
       "| **C – Adversarial manipulation** | *P3*: When ≥ ε = 10 % of rewards are replaced by adversarial values, the chosen policy will remain the same as in the clean data case in ≥ 90 % of runs (robustness). |\n",
       "\n",
       "If any prediction is violated, the corresponding assumption (A1‑A4) is falsified and the framework must be revised.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Monitoring & Update Protocol  \n",
       "\n",
       "| Component | Metric / Test | Trigger Condition | Action |\n",
       "|-----------|---------------|-------------------|--------|\n",
       "| **Distributional shift** | Kolmogorov‑Smirnov test on o_t windows (size 50) or ADWIN on reward stream. | p‑value < 0.01 **or** drift magnitude > Δ (e.g., 0.15). | Freeze current policy, recompute bootstrap L_k on the most recent window, select new π*. |\n",
       "| **Data integrity** | % of rewards beyond 3σ of rolling median; checksum anomalies. | > ε_int = 5 % anomalies. | Flag data batch, invoke manual review; if confirmed, discard batch and re‑run RPS on clean data. |\n",
       "| **Bias detection** | Disparity in L_k across protected groups (e.g., Δ_L > 0.1). | Detected bias persists over 2 consecutive windows. | Apply group‑wise regularization λ_g, re‑evaluate; if still biased, invoke escalation. |\n",
       "| **Escalation** | Tier‑1: Automated retraining. Tier‑2: Human‑in‑the‑loop review (policy ethics board). Tier‑3: Full rollback to previously validated policy π_{prev}. |\n",
       "| **Rollback** | If new policy’s L_k < L_{prev} – γ (γ = 0.05) after 30 new samples, revert to π_{prev} and log incident. |\n",
       "\n",
       "All alerts and actions are logged with timestamps for auditability.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Ethical, Legal & Social Assessment  \n",
       "\n",
       "| Issue | Stakeholder impact | Mitigation |\n",
       "|-------|-------------------|------------|\n",
       "| **Fairness** | Unequal utility across demographic groups. | Include group‑wise penalties in λ·|π|, monitor Δ_L, and enforce a fairness threshold (e.g., 80 % parity). |\n",
       "| **Transparency** | Decision makers need to understand why a policy was chosen. | Provide the full bootstrap distribution, complexity penalty, and a concise natural‑language explanation for π*. |\n",
       "| **Privacy** | Observations may contain personal data. | Store only aggregated statistics, apply differential‑privacy noise (ε≈1) to reward counts before bootstrapping. |\n",
       "| **Liability** | Wrong policy could cause harm (e.g., medical triage). | Keep a versioned audit trail; maintain a “safe fallback” policy vetted by regulators. |\n",
       "| **Adversarial exploitation** | Attackers could poison data to force a sub‑optimal policy. | Limit ε‑contamination assumption, monitor integrity metrics, and enforce rate‑limiting on data ingestion. |\n",
       "| **Regulatory compliance** | GDPR, FDA, etc. | Conduct Data Protection Impact Assessment (DPIA), ensure explainability per EU AI Act, and submit the decision rule to the relevant oversight body. |\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Justification, Strengths & Limitations  \n",
       "\n",
       "**Why RPS should work in practice**  \n",
       "* **Robustness** – By basing the decision on a lower‑confidence bound, we protect against sampling noise, limited data, and modest adversarial contamination.  \n",
       "* **Model‑agnosticism** – No need to specify a dynamics model; the method works with any observable‑reward data.  \n",
       "* **Interpretability** – The final rule is a simple policy (e.g., a shallow decision tree) with an explicit utility bound, easy to audit.  \n",
       "* **Computationally cheap** – Bootstrap + simple penalty scales linearly with the number of policies; feasible on edge devices.  \n",
       "\n",
       "**Key limitations**  \n",
       "1. **Policy library dependence** – If the optimal action lies outside 𝒫, performance degrades (violates A4).  \n",
       "2. **Bootstrap reliability under extreme heavy‑tailed noise** – Sub‑Gaussian assumption may fail, leading to overly optimistic L_k.  \n",
       "3. **Detection latency** – Drift tests need a few dozen samples; rapid shifts may cause temporary sub‑optimal actions.  \n",
       "4. **Adversarial budget** – The guarantee holds only up to the assumed ε contamination; a coordinated attack exceeding this budget can break the system.  \n",
       "\n",
       "**Failure scenarios**  \n",
       "* Highly non‑stationary environments where the reward distribution changes faster than the monitoring window.  \n",
       "* When observation space o_t is so severely corrupted that no informative signal remains (complete loss of partial observability).  \n",
       "\n",
       "---\n",
       "\n",
       "## Final Decision Rule (Executable Pseudocode)\n",
       "\n",
       "```python\n",
       "# Robust Policy Selection (RPS)\n",
       "\n",
       "import numpy as np\n",
       "from sklearn.utils import resample\n",
       "\n",
       "def rps(decision_policies, data, B=150, alpha=0.05, lam=0.01):\n",
       "    \"\"\"\n",
       "    decision_policies : list of callables π_k(o) -> a\n",
       "    data             : list of tuples (o, a, r)\n",
       "    B                : number of bootstrap replications\n",
       "    alpha            : lower‑confidence level (e.g., 0.05)\n",
       "    lam              : complexity penalty weight\n",
       "    Returns (π_star, L_star, diagnostics)\n",
       "    \"\"\"\n",
       "    # pre‑compute policy complexities (e.g., number of rules)\n",
       "    complexities = [policy.complexity() for policy in decision_policies]\n",
       "\n",
       "    # extract rewards per policy\n",
       "    rewards_by_policy = {k: [] for k in range(len(decision_policies))}\n",
       "    for (o, a, r) in data:\n",
       "        for k, π in enumerate(decision_policies):\n",
       "            if π(o) == a:                # reward belongs to policy k\n",
       "                rewards_by_policy[k].append(r)\n",
       "\n",
       "    # bootstrap lower bounds\n",
       "    lower_bounds = []\n",
       "    for k, rewards in rewards_by_policy.items():\n",
       "        if len(rewards) == 0:\n",
       "            lower_bounds.append(-np.inf)\n",
       "            continue\n",
       "        boot_means = []\n",
       "        for _ in range(B):\n",
       "            sample = resample(rewards, replace=True, n_samples=len(rewards))\n",
       "            boot_means.append(np.mean(sample))\n",
       "        Lk = np.quantile(boot_means, alpha) - lam * complexities[k]\n",
       "        lower_bounds.append(Lk)\n",
       "\n",
       "    # choose best policy\n",
       "    best_idx = int(np.argmax(lower_bounds))\n",
       "    π_star = decision_policies[best_idx]\n",
       "    L_star = lower_bounds[best_idx]\n",
       "\n",
       "    diagnostics = {\n",
       "        \"lower_bounds\": lower_bounds,\n",
       "        \"complexities\": complexities,\n",
       "        \"bootstrap_means\": boot_means  # last computed (optional)\n",
       "    }\n",
       "    return π_star, L_star, diagnostics\n",
       "```\n",
       "\n",
       "*Deploy*: run `rps` after every monitoring window (≈50 new samples). If the selected `π_star` differs from the current operational policy, trigger the **Update Protocol** (Section 4).  \n",
       "\n",
       "---  \n",
       "\n",
       "*End of answer.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Competitor 3\n",
    "\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "c3_start_time = time.time()\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "c3_end_time = time.time()\n",
    "c3_duration = c3_end_time - c3_start_time\n",
    "\n",
    "print(f\"Time taken for {model_name}: **{c3_duration:.4f} seconds**\\n\")\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "durations.append(c3_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5e174630-f3b5-426c-8f75-47c72fab8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-5-mini', 'gemini-2.5-flash', 'openai/gpt-oss-120b']\n",
      "[63.00679039955139, 39.52344059944153, 6.044901609420776]\n"
     ]
    }
   ],
   "source": [
    "print(competitors)\n",
    "print(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "020b2c02-4f26-4bd4-9050-7d005e11a051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td># Robust Decision Framework for Selecting a Si...</td>\n",
       "      <td>63.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-2.5-flash</td>\n",
       "      <td>The following robust decision framework is des...</td>\n",
       "      <td>39.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai/gpt-oss-120b</td>\n",
       "      <td>## 1. Model‑agnostic Decision Rule  \\n**Robust...</td>\n",
       "      <td>6.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model                                             Answer  \\\n",
       "0           gpt-5-mini  # Robust Decision Framework for Selecting a Si...   \n",
       "1     gemini-2.5-flash  The following robust decision framework is des...   \n",
       "2  openai/gpt-oss-120b  ## 1. Model‑agnostic Decision Rule  \\n**Robust...   \n",
       "\n",
       "   Time (s)  \n",
       "0    63.007  \n",
       "1    39.523  \n",
       "2     6.045  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "competitors_answers = pd.DataFrame({\n",
    "    \"Model\": competitors,\n",
    "    \"Answer\": answers,\n",
    "    \"Time (s)\": [round(d, 3) for d in durations]\n",
    "})\n",
    "\n",
    "competitors_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aeac17ea-ed6e-42e2-b1b2-50ef45226b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Competitor 1: **gpt-5-mini**\n",
       "# Robust Decision Framework for Selecting a Single Policy under Extreme Uncertainty\n",
       "\n",
       "Summary: a model-agnostic, implementable rule (Robust Lower-bound Selection, RLS) that (a) evaluates candidate policies under an ensemble of plausible perturbations (noise, shifts, adversarial manipulations), (b) chooses the policy that maximizes a conservative performance metric (worst-case lower confidence bound or CVaR) while penalizing complexity, (c) produces an interpretable rationale and monitoring/rollback plan.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Final decision rule (algorithm: Robust Lower-bound Selection — RLS)\n",
       "\n",
       "Inputs\n",
       "- Candidate policy set P = {π1,...,πk} (can be rules, parametric policies, or black-box policies).\n",
       "- History dataset D = {(x_i, a_i, r_i, t_i, meta_i)} (covariates x, action a, reward/outcome r, timestamp t, metadata).\n",
       "- Baseline policy π0 (safe fallback).\n",
       "- Utility function U(π; D) (e.g., expected reward, harm-weighted utility).\n",
       "- Uncertainty generators G = {g1,...,gm} describing plausible perturbations (noise levels, covariate shifts, label corruption, adversarial fraction ε).\n",
       "- Off-policy evaluator E (IPS with clipped weights or Doubly Robust), bootstrap count B, confidence level α (e.g., 0.05), CVaR tail τ (optional, e.g., 0.1).\n",
       "- Complexity penalty C(π) (e.g., model size, interpretability cost), weight λ.\n",
       "- Trigger thresholds for monitoring (see §4).\n",
       "\n",
       "Outputs\n",
       "- Selected policy π* and interpretability report R (LCBs/CVaR across scenarios, sensitivity summary, failure modes).\n",
       "- Deployment plan & rollback steps.\n",
       "\n",
       "Procedure\n",
       "1. Data sanitization:\n",
       "   - Basic integrity checks (missingness, duplicates, signature/hashes), remove/flag corrupted rows.\n",
       "   - Estimate propensity scores p(a|x) if needed for IPS; apply clipping at w_max.\n",
       "\n",
       "2. Build perturbation scenarios:\n",
       "   - For each g ∈ G, define parameterized perturbations (e.g., additive noise σ, covariate shift magnitude δ, label flip rate η, fraction ε of adversarial examples).\n",
       "   - Use domain knowledge to bound ranges (conservative priors).\n",
       "\n",
       "3. Bootstrap evaluation:\n",
       "   - For b = 1..B:\n",
       "     - Resample D_b from D (with stratification if needed).\n",
       "     - For each scenario s generated from G:\n",
       "       - Apply perturbation s to D_b → D_{b,s}.\n",
       "       - For each candidate policy π:\n",
       "         - Compute off-policy estimate U_{b,s}(π) using E on D_{b,s}; record estimator variance.\n",
       "   - This yields distribution of U across (b,s) for each π.\n",
       "\n",
       "4. Compute robust score per π:\n",
       "   - Option A (Lower-confidence bound): LCB(π) = empirical α-quantile of U_{b,s}(π) across all (b,s).\n",
       "   - Option B (CVaR): CVaR_τ(π) = mean of worst τ fraction of U_{b,s}(π).\n",
       "   - RobustScore(π) = LCB(π) − λ·C(π) (or CVaR_τ(π) − λ·C(π)).\n",
       "   - Optionally compute worst-case regret relative to baseline: WorstRegret(π) = max_{b,s} (U_{b,s}(π0) − U_{b,s}(π)).\n",
       "\n",
       "5. Selection:\n",
       "   - Choose π* = argmax_π RobustScore(π), subject to constraint WorstRegret(π) ≤ R_max (set R_max per risk tolerance). If no π meets constraints, default to baseline π0.\n",
       "\n",
       "6. Rationale and interpretability output R:\n",
       "   - Report LCB/CVaR and median for chosen π*, complexity cost, most damaging perturbation scenarios, sensitivity charts, and policy decision rule summary.\n",
       "   - Provide simple explanation: e.g., \"π* selected because its 95% lower bound on expected utility under plausible perturbations is greatest and worst-case regret vs baseline ≤ X.\"\n",
       "\n",
       "7. Deploy with guardrails:\n",
       "   - Deploy π* in staged rollout (canary, A/B with heavy monitoring), see §4 for triggers and rollback.\n",
       "\n",
       "Implementation notes\n",
       "- Off-policy evaluation: use Doubly Robust or weighted IPS; clip importance weights and report sensitivity to clipping.\n",
       "- Keep candidate policies simple where possible (rules, small trees, linear policies) to improve interpretability and reduce overfitting.\n",
       "- Choose B (e.g., 500) and number of scenarios m pragmatically—enough for stable estimates given data size.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Assumptions, data requirements, and empirical test plan\n",
       "\n",
       "Assumptions (explicit)\n",
       "- A1: Utility function U correctly captures stakeholder objectives (or a defensible proxy exists).\n",
       "- A2: Candidate policies P contain at least one safe fallback π0.\n",
       "- A3: Historical data D is informative for the short-term behavior of the system (coverage of relevant covariate-action space).\n",
       "- A4: Perturbation family G contains realistic bounds for noise, shift, and adversary strength (conservative, but plausible).\n",
       "- A5: Off-policy evaluation bias is tolerable with clipping/DR corrections (no extreme propensities).\n",
       "\n",
       "Data requirements (minimum)\n",
       "- N_total: at least a few hundred independent episodes per key stratum; if scarce, rely more on conservative G and simpler policies.\n",
       "- Metadata: timestamps, provenance, and integrity checks.\n",
       "- Feature coverage: key covariates that influence action-outcome relationships must appear in D.\n",
       "- Baseline behavior data for π0 to compute regret.\n",
       "\n",
       "Plan to empirically test assumptions (hypothetical scenarios)\n",
       "- Create synthetic datasets that mimic domain features and labeled outcomes; vary noise, shift, adversarial fractions.\n",
       "- Tests:\n",
       "  - Coverage test (A3): remove certain covariate strata from training data; measure IPS variance and LCB widening.\n",
       "  - Sensitivity to G (A4): expand/shrink perturbation ranges; confirm selection stability (π* stable under reasonable changes).\n",
       "  - OPE validity (A5): compare off-policy estimates to on-policy returns in small controlled online rollouts (canary) to validate E calibration.\n",
       "- Acceptance criteria:\n",
       "  - If empirical LCBs consistently underestimate true on-policy canary returns by >δ, recalibrate E or tighten rolling deployment.\n",
       "  - If candidate selection flips across many plausible G changes, increase conservatism (larger λ or stricter WorstRegret).\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Falsifiable predictions (three regimes)\n",
       "\n",
       "Define metric M = realized average utility per episode measured on holdout/on-policy canary.\n",
       "\n",
       "Regime A — Stable, modest noise\n",
       "- Prediction A1: Within T_A (e.g., 2 weeks or 10k episodes), M(π*) ≥ median LCB(π*) + ε_A (ε_A small, e.g., 0.02 utility units).\n",
       "- Falsifiability: if M(π*) < LCB(π*) − δ_A, prediction fails.\n",
       "\n",
       "Regime B — Non-stationarity/concept drift\n",
       "- Prediction B1: M(π*) will degrade gradually; degradation slope s satisfies |ΔM/Δt| ≤ s_max over initial window W (we expect detect-and-update triggers before performance drops below baseline).\n",
       "- Falsifiability: if M(π*) drops abruptly below baseline π0 without preceding drift signals (distributional detectors not triggered), prediction fails.\n",
       "\n",
       "Regime C — Adversarial manipulation\n",
       "- Prediction C1: Under adversarial fraction ≤ ε_adversary (modeled in G), worst-case observed M(π*) ≥ WorstCase_LCB(π*) − ε_C (conservative), and monitoring will detect integrity anomalies (metadata tampering, abnormal reward patterns) within detection window T_C.\n",
       "- Falsifiability: if adversary-induced outcomes produce M(π*) significantly below WorstCase_LCB and no integrity flags are raised in T_C, prediction fails.\n",
       "\n",
       "Each prediction is falsifiable by comparing on-policy logged metrics to predicted intervals and by inspecting detector triggers.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Monitoring and update protocol (detection, escalation, rollback)\n",
       "\n",
       "Continuous monitoring components\n",
       "- Data-distribution monitors:\n",
       "  - Covariate shift: population stability index (PSI) and KL divergence on key features; threshold θ_cov (e.g., PSI > 0.2).\n",
       "  - Label/Outcome shift: compare recent outcome distribution to baseline with Earth Mover’s Distance; threshold θ_lab.\n",
       "- Model/performance monitors:\n",
       "  - Utility drift: rolling average of utility M with statistical control charts (CUSUM/Shewhart) at significance α_mon.\n",
       "  - Off-policy vs on-policy divergence: compare OPE predictions to canary actuals.\n",
       "- Integrity/anomaly monitors:\n",
       "  - Metadata validation (signatures, device IDs), outlier detection on timestamps and event rates.\n",
       "  - Adversarial detectors: sudden cluster of contradictory examples, reward spikes, or disproportionate error rates.\n",
       "- Fairness/bias monitors:\n",
       "  - Disparate impact (ratio metrics), subgroup utilities, intersectional checks; thresholds set by policy.\n",
       "\n",
       "Trigger conditions (examples)\n",
       "- Tier 1 (warning): PSI > θ1 or utility drift p-value < 0.05 → notify operations and data science.\n",
       "- Tier 2 (action required): PSI > θ2 or utility drop > X% vs baseline OR integrity anomaly → pause new exposures to π*; initiate canary rollback to π0 for affected segments.\n",
       "- Tier 3 (urgent rollback): evidence of data tampering or targeted harm (meet legal/ethical stop criteria) → immediate full rollback to π0 and incident response.\n",
       "\n",
       "Escalation path\n",
       "- Tier 1: Automated alert → data science on-call inspects dashboards within 4 hours.\n",
       "- Tier 2: Convene cross-functional response (ops, DS, legal, compliance) within 2 hours; restrict traffic to canary; collect forensic logs.\n",
       "- Tier 3: Immediate cessation; notify leadership and regulators as required; start post-incident review.\n",
       "\n",
       "Rollback & recovery procedures\n",
       "- Pre-deploy: ensure π0 is always deployable, versioned, and tested; keep feature transforms consistent.\n",
       "- Immediate rollback: switch routing to π0; preserve logs; mark affected data for quarantine.\n",
       "- Post-rollback: root-cause analysis, patch G and monitoring thresholds, retrain or replace candidate set if needed.\n",
       "- Audit trail: full logging of decision, metrics, time of rollback, and rationale; share with stakeholders.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Ethical, legal, and social implications & mitigation\n",
       "\n",
       "Key stakeholder impacts\n",
       "- Users potentially harmed by mistaken decisions (safety-critical domains).\n",
       "- Subpopulations disproportionately affected (bias).\n",
       "- Data subjects’ privacy and consent.\n",
       "- Organizational/legal exposure (regulation compliance).\n",
       "\n",
       "Risks\n",
       "- Distributional bias causing disproportionate harms.\n",
       "- Adversarial exploitation leading to targeted harms.\n",
       "- Over-reliance on automated decision with insufficient human oversight.\n",
       "- Privacy leaks from logs or models.\n",
       "\n",
       "Concrete mitigation strategies\n",
       "- Fairness constraints: enforce minimum subgroup utility floors during selection (add constraints to WorstRegret or per-group LCB≥g_min).\n",
       "- Transparency & explanation: produce R reporting interpretable rationale for chosen policy; publish summary for stakeholders.\n",
       "- Human-in-the-loop: require human approval for high-risk deployments and for Tier 2+ escalations.\n",
       "- Data governance: strict provenance, access controls, hashing/signatures for integrity, and retention policies.\n",
       "- Privacy protections: minimize sensitive attributes in decisioning; apply differential privacy or noise to analytics when needed.\n",
       "- Adversarial hardening: red-team tests, simulated adversary scenarios in G, input sanity checks.\n",
       "- Legal compliance: consult counsel for sector rules (e.g., GDPR, HIPAA); implement opt-out and data subject rights flows.\n",
       "- Independent audits: periodic external fairness/safety audits and public reporting where appropriate.\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Justification, expected performance, and limitations\n",
       "\n",
       "Why this will perform well in practice\n",
       "- Conservative selection (LCB/CVaR) reduces chance of worst-case catastrophic outcomes under uncertainty—suitable when stakes are high.\n",
       "- Model-agnostic: works with rules, simple models, or complex policies; off-policy evaluation avoids extensive online exploration.\n",
       "- Bootstrapping + scenario perturbations produce robust uncertainty quantification even with limited data.\n",
       "- Complexity penalty and preference for simpler policies reduce overfitting and improve interpretability.\n",
       "\n",
       "Key limitations and failure modes\n",
       "- Conservatism: may select suboptimal policy for benign environments (opportunity cost).\n",
       "- Unmodeled \"unknown unknowns\": if real-world perturbations lie outside G, selection may fail.\n",
       "- Dependence on OPE accuracy: heavy bias in E (poor propensities, extreme covariate mismatch) undermines estimates.\n",
       "- Candidate-policy coverage: if P lacks any robust good policy, selection cannot fix it—requires human design loop.\n",
       "- Data scarcity: with very small N, LCBs are wide and decision may default to baseline frequently.\n",
       "- Sophisticated adversary: if adversary can adapt in deployment beyond modeled capacity, monitoring might lag.\n",
       "\n",
       "Mitigations for limitations\n",
       "- Periodic expansion of G via adversarial red-teaming; incorporate rapid canary cycles to validate OPE.\n",
       "- Maintain policy development pipeline to generate new candidates when selection repeatedly returns π0.\n",
       "- Combine with conservative online learning only under strict safety constraints.\n",
       "\n",
       "---\n",
       "\n",
       "## Quick parameter guidance (practical defaults)\n",
       "- B (bootstrap): 200–1000 depending on compute.\n",
       "- α for LCB: 0.05 (95% lower bound).\n",
       "- CVaR τ: 0.1 if risk-averse.\n",
       "- Importance weight clipping w_max: 10.\n",
       "- Complexity penalty λ: tuned so simple policy preferred unless complex policy improves LCB by >δ.\n",
       "- Monitoring windows: short-term (hourly) for integrity; medium-term (daily) for utility drift.\n",
       "\n",
       "---\n",
       "\n",
       "## Deliverables produced by the framework\n",
       "- Selected policy π* with decision report R (LCBs, worst-case scenarios, sensitivity).\n",
       "- Monitoring dashboard with PSI, utility charts, fairness metrics, integrity logs.\n",
       "- Escalation playbook and rollback scripts (automated routing to π0).\n",
       "- Test suite of simulated scenarios used to validate assumptions.\n",
       "\n",
       "---\n",
       "\n",
       "This framework is concise, implementable with limited data, and designed to minimize overfitting while producing interpretable justification for the chosen policy. It trades some short-term performance for robustness and safety under extreme uncertainty; use domain expertise to set G, thresholds, and the baseline policy.\n",
       "\n",
       "### Competitor 2: **gemini-2.5-flash**\n",
       "The following robust decision framework is designed for selecting a single policy under extreme uncertainty, partial observability, and with noisy/adversarial data.\n",
       "\n",
       "---\n",
       "\n",
       "### **1. Model-Agnostic Decision Rule: Robust Scenario-Based Policy Selection (RSBPS)**\n",
       "\n",
       "The RSBPS framework prioritizes robustness and interpretability over predictive accuracy, which is highly challenging under extreme uncertainty and adversarial conditions. It identifies a policy that minimizes the maximum regret across a diverse set of plausible, stressful, and adversarial future scenarios.\n",
       "\n",
       "**Input:**\n",
       "*   A finite set of candidate policies: $P = \\{p_1, p_2, \\dots, p_k\\}$.\n",
       "*   A set of critical uncertainties and features derived from the partially observable, noisy data: $U = \\{u_1, u_2, \\dots, u_n\\}$. These represent key variables or processes that significantly impact policy outcomes but are subject to high uncertainty or manipulation.\n",
       "*   A performance metric: $M(p_i, s_j)$, a scalar value representing the outcome (e.g., cost, safety, efficiency, impact) of applying policy $p_i$ under scenario $s_j$. Assume $M$ is defined such that *lower values are better* (e.g., lower cost, fewer failures). If higher values are better, adjust regret calculation accordingly.\n",
       "\n",
       "**Algorithm:**\n",
       "\n",
       "1.  **Scenario Construction (S):**\n",
       "    *   **Define Plausible & Adversarial Scenarios:** Based on current and historical data, expert knowledge, and known adversarial tactics, construct a comprehensive set of future scenarios $S = \\{s_1, s_2, \\dots, s_m\\}$. Each $s_j$ describes a specific realization of the critical uncertainties $U$.\n",
       "    *   **Types of Scenarios:**\n",
       "        *   **Nominal:** Most likely or expected conditions (e.g., stable data-generating process with modest noise).\n",
       "        *   **Stress-Test:** Extreme but plausible conditions (e.g., significant but random data noise, unexpected market shifts, resource scarcity).\n",
       "        *   **Adversarial:** Conditions where data is actively manipulated or system behavior is targeted (e.g., data poisoning, spoofing, denial-of-service, concept drift induced by an adversary).\n",
       "        *   **Drift/Non-stationarity:** Scenarios explicitly modeling shifts in data distributions or underlying processes.\n",
       "        *   **Black Swan (if conceivable):** Highly improbable but high-impact events.\n",
       "    *   *Rationale:* This step integrates domain expertise and adversarial thinking directly into the decision process, bypassing the need for complex predictive models.\n",
       "\n",
       "2.  **Policy Performance Evaluation:**\n",
       "    *   For each policy $p_i \\in P$ and each scenario $s_j \\in S$, estimate or simulate the performance metric $M(p_i, s_j)$. This step may involve simplified models, expert elicitation, or historical data-driven simulations tailored to each scenario's conditions.\n",
       "    *   *Rationale:* Directly assesses policy outcomes under diverse conditions, rather than relying on a single, potentially flawed model of the environment.\n",
       "\n",
       "3.  **Regret Calculation:**\n",
       "    *   For each scenario $s_j$, identify the *optimal* performance among all candidate policies: $M_{optimal}(s_j) = \\min_{p_i \\in P} \\{M(p_i, s_j)\\}$.\n",
       "    *   Calculate the *scenario-specific regret* for each policy $p_i$ under scenario $s_j$: $R(p_i, s_j) = M(p_i, s_j) - M_{optimal}(s_j)$. This measures how much worse policy $p_i$ performs compared to the best possible policy *for that specific scenario*.\n",
       "    *   *Rationale:* Focuses on relative performance, which is more robust than absolute performance in highly uncertain environments.\n",
       "\n",
       "4.  **Max Regret Identification:**\n",
       "    *   For each policy $p_i$, identify its *maximum regret* across all scenarios: $MaxRegret(p_i) = \\max_{s_j \\in S} \\{R(p_i, s_j)\\}$. This represents the worst-case outcome for policy $p_i$ relative to the optimal choice for any given scenario.\n",
       "    *   *Rationale:* Explicitly identifies and quantifies the \"least worst\" performance, preparing for pessimistic outcomes.\n",
       "\n",
       "5.  **Policy Selection (Minimax Regret Criterion):**\n",
       "    *   Select the policy $p^*$ that minimizes the maximum regret:\n",
       "        $p^* = \\arg \\min_{p_i \\in P} \\{MaxRegret(p_i)\\}$\n",
       "    *   *Rationale:* This \"minimax regret\" strategy selects the policy that guarantees the best possible outcome under the worst possible scenario (relative to the optimal choice for that scenario). It's a robust choice when facing deep uncertainty and potential adversaries.\n",
       "\n",
       "**Interpretable Rationale:** The selected policy $p^*$ is chosen because it is the most robust option, minimizing the maximum potential \"missed opportunity\" or \"sub-optimality\" across all considered scenarios. Its rationale is directly tied to its comparative performance across the explicitly defined plausible, stressful, and adversarial futures.\n",
       "\n",
       "---\n",
       "\n",
       "### **2. Assumptions, Data Requirements, and Empirical Testing Plan**\n",
       "\n",
       "**Assumptions:**\n",
       "*   **Existence of Candidate Policies:** A finite, well-defined set of policies exists for consideration.\n",
       "*   **Feasibility of Scenario Definition:** It's possible to define a sufficiently rich and relevant set of scenarios (including adversarial ones) that capture the critical uncertainties.\n",
       "*   **Measurable Performance:** A quantifiable, agreed-upon performance metric $M$ can be defined for any policy under any scenario.\n",
       "*   **Bounded Rationality of Adversaries (for adversarial scenarios):** While adversaries are malicious, their capabilities and objectives are not entirely unknowable; there are limits to their manipulation.\n",
       "*   **Observable Indicators (even if noisy):** Despite partial observability and noise, some indicators or features (e.g., changes in correlation, distribution shifts, specific attack signatures) can be detected to inform scenario updates and monitoring.\n",
       "\n",
       "**Data Requirements:**\n",
       "*   **Historical Policy Performance Data:** Records of how past policies performed under varying conditions (if available).\n",
       "*   **Feature Data:** Current and historical data streams relevant to the critical uncertainties $U$, even if noisy or sparse. This informs scenario definition and monitoring.\n",
       "*   **Expert Elicitation:** Input from domain experts on plausible scenarios, potential adversarial tactics, and policy outcomes in specific situations.\n",
       "*   **Adversarial Intelligence:** Data on known attack vectors, adversarial capabilities, and objectives from security reports or threat intelligence.\n",
       "*   **Policy Specifications:** Detailed descriptions of each candidate policy $p_i$.\n",
       "\n",
       "**Empirical Testing Plan (Hypothetical Scenarios):**\n",
       "\n",
       "1.  **Regime A: Stable Data-Generating Process with Modest Noise**\n",
       "    *   **Hypothetical Setup:** Simulate a stable environment where underlying processes are constant, and data noise follows a well-behaved distribution (e.g., Gaussian). Candidate policies are designed for this environment.\n",
       "    *   **Test:**\n",
       "        *   Define scenarios including the nominal stable state and slight variations due to noise.\n",
       "        *   Run RSBPS to select a policy.\n",
       "        *   Generate multiple independent data runs from the stable process.\n",
       "        *   Evaluate the selected policy's performance.\n",
       "    *   **Expected Outcome:** RSBPS should consistently select a policy close to the empirically optimal policy for the stable regime, demonstrating its ability to find a robust choice even when uncertainty is minimal.\n",
       "\n",
       "2.  **Regime B: Non-stationarity and Concept Drift**\n",
       "    *   **Hypothetical Setup:** Introduce a gradual or abrupt shift in the data-generating process (e.g., changing correlations between features, altered underlying distributions) after an initial stable period.\n",
       "    *   **Test:**\n",
       "        *   Initially, use RSBPS under the stable regime.\n",
       "        *   Introduce the concept drift.\n",
       "        *   Use the monitoring protocol (Section 4) to detect the shift.\n",
       "        *   Upon detection, re-run RSBPS with *updated scenarios* that incorporate the detected drift.\n",
       "        *   Compare the performance of the *initial* selected policy versus the *re-selected* policy after drift detection.\n",
       "    *   **Expected Outcome:** The monitoring system should reliably detect the drift. RSBPS, when re-run with updated scenarios, should select a different, more appropriate policy that performs significantly better in the drifted environment than the initial policy.\n",
       "\n",
       "3.  **Regime C: Adversarial Manipulation**\n",
       "    *   **Hypothetical Setup:** Simulate an environment where a subset of input data is maliciously manipulated (e.g., data poisoning, feature injection) by an adversary aiming to degrade system performance or steer policy choice.\n",
       "    *   **Test:**\n",
       "        *   Define adversarial scenarios within RSBPS, explicitly modeling the adversary's capabilities and goals.\n",
       "        *   Run RSBPS.\n",
       "        *   Simulate the adversarial attacks on the data streams.\n",
       "        *   Evaluate the selected policy's performance under these attacks, and also evaluate other \"less robust\" policies (e.g., one selected by a purely predictive model without adversarial scenarios).\n",
       "    *   **Expected Outcome:** The policy selected by RSBPS should demonstrate greater resilience (i.e., less degradation in performance) against the simulated adversarial attacks compared to policies not chosen with adversarial scenarios in mind. Monitoring should detect integrity issues.\n",
       "\n",
       "---\n",
       "\n",
       "### **3. Falsifiable Predictions**\n",
       "\n",
       "1.  **Regime A (Stable Data-Generating Process with Modest Noise):**\n",
       "    *   **Prediction:** If the data-generating process is stable with modest noise, the RSBPS framework will select a policy whose average performance over 100 independent trials falls within 5% of the empirically optimal policy's average performance, and the selected policy will remain unchanged for at least 90% of re-evaluations (if no monitoring triggers).\n",
       "    *   **Falsification:** If the chosen policy's average performance is consistently more than 5% worse than the empirically optimal, or if the policy selection frequently changes without external triggers, the prediction is falsified.\n",
       "\n",
       "2.  **Regime B (Non-stationarity and Concept Drift):**\n",
       "    *   **Prediction:** When a significant concept drift (defined as a statistical distribution shift detectable by a Kolmogorov-Smirnov test at p<0.01 between pre- and post-drift data windows) occurs, the monitoring protocol will trigger a re-evaluation within 3 data cycles, leading to RSBPS selecting a *different* policy from the original one in at least 80% of drift events, and this new policy will perform at least 10% better than the original policy would have in the post-drift environment.\n",
       "    *   **Falsification:** If drift is not detected, or if the selected policy does not change, or if the new policy does not significantly outperform the old one post-drift, the prediction is falsified.\n",
       "\n",
       "3.  **Regime C (Adversarial Manipulation):**\n",
       "    *   **Prediction:** If 10% of input data is subjected to known adversarial poisoning attacks (e.g., label flipping, specific feature manipulation), the policy selected by RSBPS (which explicitly considers adversarial scenarios) will experience at most a 15% degradation in its performance metric, whereas a policy selected without considering adversarial scenarios would experience a degradation exceeding 30%. The monitoring system will also flag data integrity issues in over 95% of attack instances.\n",
       "    *   **Falsification:** If the RSBPS-selected policy degrades by more than 15%, or if the non-adversarially chosen policy degrades by less than 30%, or if the monitoring system fails to detect most attacks, the prediction is falsified.\n",
       "\n",
       "---\n",
       "\n",
       "### **4. Monitoring and Update Protocol**\n",
       "\n",
       "This protocol ensures the continued relevance and safety of the selected policy.\n",
       "\n",
       "**Key Components:**\n",
       "*   **Data Quality & Integrity Monitors:**\n",
       "    *   **Scope:** Raw input data, processed features, intermediary model outputs (if any).\n",
       "    *   **Checks:** Value range, completeness, consistency, statistical anomalies (e.g., sudden spikes/drops, outliers), feature correlations, entropy levels.\n",
       "    *   **Techniques:** Control charts (e.g., CUSUM, EWMA), statistical tests (e.g., Z-test for means, Chi-squared for distributions), anomaly detection algorithms (e.g., Isolation Forest, Autoencoders).\n",
       "*   **Distributional Shift Monitors:**\n",
       "    *   **Scope:** Key feature distributions, target variable distribution (if applicable), relationship between features and outcomes.\n",
       "    *   **Checks:** Change in mean, variance, skewness, kurtosis; shifts in multivariate distributions.\n",
       "    *   **Techniques:** Drift detection algorithms (e.g., ADWIN, DDM), statistical tests (e.g., Kolmogorov-Smirnov, Jensen-Shannon divergence, Adversarial validation), concept drift detectors.\n",
       "*   **Performance Monitors:**\n",
       "    *   **Scope:** Actual observed policy performance against the chosen metric $M$.\n",
       "    *   **Checks:** Deviations from expected performance, increasing variance, unexpected outcomes.\n",
       "    *   **Techniques:** A/B testing (if multiple policies are run in parallel in sub-segments), backtesting against historical data, control charts on performance metrics.\n",
       "*   **Bias Monitors:**\n",
       "    *   **Scope:** Disparate impact across defined demographic groups or critical sub-populations, fairness metrics (e.g., Equal Opportunity, Demographic Parity).\n",
       "    *   **Checks:** Discrepancies in performance $M$ or other impact metrics across groups.\n",
       "    *   **Techniques:** Regular auditing of group-specific outcomes.\n",
       "\n",
       "**Trigger Conditions:**\n",
       "*   **Severity 1 (Warning):**\n",
       "    *   Minor statistical anomalies or deviations (e.g., 1-sigma deviation in a control chart).\n",
       "    *   Small, localized shifts in feature distributions that don't yet impact overall performance.\n",
       "    *   Slight increase in uncertainty estimates.\n",
       "    *   *Action:* Automated alert to monitoring team, increased logging, deeper automated diagnostics.\n",
       "*   **Severity 2 (Alert):**\n",
       "    *   Persistent or significant statistical anomalies (e.g., 2-sigma deviation).\n",
       "    *   Detected distributional shift (e.g., K-S test p-value < 0.05).\n",
       "    *   Observed performance degradation (e.g., $M$ deviates by 5-10% from expected).\n",
       "    *   *Action:* Immediate alert to human operators/experts, detailed diagnostic report, initiation of a review process.\n",
       "*   **Severity 3 (Critical):**\n",
       "    *   Catastrophic data integrity issues (e.g., data source offline, massive corruption).\n",
       "    *   Major concept drift or non-stationarity (e.g., K-S test p-value < 0.01, ADWIN/DDM triggers).\n",
       "    *   Significant performance degradation (e.g., $M$ deviates by >10% from expected, or violates safety thresholds).\n",
       "    *   Detection of known adversarial attack signatures.\n",
       "    *   *Action:* Emergency protocol initiation, immediate policy review, potential temporary policy rollback.\n",
       "\n",
       "**Escalation Paths:**\n",
       "1.  **Automated Anomaly Detection (Severity 1):** System generates internal alerts and logs.\n",
       "2.  **Human Monitoring Team (Severity 2):** Designated team investigates alerts, performs root cause analysis, and determines if a policy re-evaluation is needed.\n",
       "3.  **Expert Panel/Stakeholder Review (Severity 3):** If critical issues arise, or a policy change is proposed, a panel of domain experts and key stakeholders (including ethical review board members) convenes to review diagnostics, assess new scenarios, and approve policy re-selection or emergency measures. This includes reviewing potential new adversarial scenarios.\n",
       "4.  **Policy Re-evaluation:** If drift or adversarial activity is confirmed, the RSBPS framework is re-run with updated scenarios reflecting the new reality, potentially leading to a new policy selection.\n",
       "\n",
       "**Rollback Procedures:**\n",
       "*   **Temporary Rollback:** In a critical situation, revert to a previously known \"safe\" or default policy. This policy should be pre-approved as a stable, albeit potentially sub-optimal, fallback.\n",
       "*   **Data Rollback/Sanitization:** If data integrity is compromised, attempt to quarantine or rollback to a clean data state, or apply specific sanitization techniques.\n",
       "*   **Version Control for Policies:** All policies and framework configurations are under version control, allowing easy reversion to previous stable versions.\n",
       "*   **Continuous Monitoring Post-Rollback:** Intensified monitoring of the rollback policy and data until confidence is restored and a new stable policy is selected.\n",
       "\n",
       "---\n",
       "\n",
       "### **5. Ethical, Legal, and Social Implications (ELSI)**\n",
       "\n",
       "**Stakeholders:**\n",
       "*   **Decision-makers:** Responsible for policy selection and outcomes.\n",
       "*   **Impacted Individuals/Groups:** Those directly affected by the policy's implementation (e.g., customers, citizens, employees).\n",
       "*   **Adversaries:** Entities attempting to manipulate the system (indirectly affected by countermeasures).\n",
       "*   **Regulators/Oversight Bodies:** Enforcing legal and ethical guidelines.\n",
       "*   **Developers/Operators:** Building and maintaining the system.\n",
       "\n",
       "**Potential Biases:**\n",
       "*   **Scenario Definition Bias:** Scenarios may inadvertently overlook critical edge cases or vulnerabilities, especially for marginalized groups, or be biased by the limited imagination/experience of experts. Adversarial scenarios might over-emphasize known threats and miss novel ones.\n",
       "*   **Performance Metric Bias:** The chosen metric $M$ might implicitly favor certain outcomes or groups over others, failing to capture holistic impacts or being susceptible to \"goodhart's law\" (when a measure becomes a target, it ceases to be a good measure).\n",
       "*   **Data Collection Bias:** Historical data used for scenario calibration or monitoring might reflect existing societal biases, leading to policies that perpetuate inequities. Partially observable data exacerbates this as critical variables might be missing or skewed.\n",
       "*   **Expert Elicitation Bias:** Experts' judgments can be influenced by personal biases, overconfidence, or limited perspectives.\n",
       "*   **Rollback Bias:** Default/fallback policies might not be equally fair or effective for all groups, potentially exacerbating existing biases during crisis.\n",
       "\n",
       "**Concrete Mitigation Strategies:**\n",
       "\n",
       "1.  **Diverse Scenario Teams & Red Teaming:**\n",
       "    *   **Strategy:** Form scenario construction teams with diverse backgrounds, expertise, and perspectives. Actively involve \"red teams\" (including those with adversarial mindsets or from marginalized groups) to identify overlooked vulnerabilities, biases, and adversarial attack vectors. Conduct \"pre-mortems\" to identify ways the system could fail.\n",
       "    *   **Mitigation:** Reduces scenario definition bias and enhances the robustness against novel adversarial threats and ethical blind spots.\n",
       "\n",
       "2.  **Multi-Objective & Equity-Aware Performance Metrics:**\n",
       "    *   **Strategy:** Define performance using a set of weighted metrics, explicitly including fairness metrics (e.g., disparate impact, equal opportunity) alongside operational metrics. Ensure transparent justification for weighting. Conduct regular audits of these metrics across different demographic or critical sub-groups.\n",
       "    *   **Mitigation:** Addresses performance metric bias and ensures equitable outcomes are considered during policy selection.\n",
       "\n",
       "3.  **Data Audits and Bias Detection Tools:**\n",
       "    *   **Strategy:** Implement automated and manual processes to audit data sources for representational biases, historical inequities, and adversarial tampering. Use fairness-aware AI tools to detect and quantify biases in data distributions. Augment limited data with synthetic, debiased data or transfer learning from more diverse datasets, when appropriate and safe.\n",
       "    *   **Mitigation:** Identifies and potentially corrects data collection bias, making the information informing scenarios more reliable and equitable.\n",
       "\n",
       "4.  **Transparent Decision Records & Justification:**\n",
       "    *   **Strategy:** Maintain a clear, human-readable log of all scenarios considered, their assessed impacts, and the explicit rationale for the chosen policy ($p^*$ minimizes MaxRegret across $S$). This includes documentation of expert inputs and their justifications.\n",
       "    *   **Mitigation:** Provides interpretability and accountability, allowing stakeholders to understand *why* a particular policy was chosen, facilitating ethical review and legal compliance.\n",
       "\n",
       "5.  **Stakeholder Engagement & Ethical Review Board:**\n",
       "    *   **Strategy:** Establish an independent ethical review board (ERB) composed of ethicists, legal experts, community representatives, and technical experts. This board reviews scenarios, performance metrics, and policy selections, especially during critical escalations. Engage affected communities in understanding policy impacts.\n",
       "    *   **Mitigation:** Ensures broad ethical, legal, and social considerations are integrated into the framework, providing external oversight and legitimacy.\n",
       "\n",
       "---\n",
       "\n",
       "### **6. Justification, Limitations, and Failure Scenarios**\n",
       "\n",
       "**Justification for Practical Performance:**\n",
       "\n",
       "The RSBPS framework is likely to perform well in the specified context due to its inherent design principles:\n",
       "1.  **Robustness to Uncertainty:** By focusing on minimax regret across a wide range of scenarios (including extreme and adversarial ones), it explicitly guards against worst-case outcomes rather than hoping for best-case predictions. This is crucial when uncertainty is extreme, and predictions are unreliable.\n",
       "2.  **Interpretability:** The rationale for selecting a policy is derived directly from its performance across well-defined scenarios. This transparency allows stakeholders to understand the trade-offs and risks, fostering trust and facilitating human oversight and intervention.\n",
       "3.  **Resilience to Adversarial Data:** Incorporating adversarial scenarios directly into the decision-making process forces the selection of policies that are inherently more resilient to manipulation, rather than attempting to filter out all adversarial data (which is often an impossible task).\n",
       "4.  **Limited Data & Model Agnosticism:** It does not require vast amounts of historical data for complex model training. Instead, it leverages available data to inform scenario definition, and expert knowledge to assess policy outcomes within those scenarios. This avoids overfitting to sparse or noisy data.\n",
       "5.  **Built-in Monitoring & Adaptability:** The comprehensive monitoring protocol, combined with explicit re-evaluation triggers and rollback procedures, ensures that the system can detect changes in the environment (including adversarial attacks or concept drift) and adapt the policy accordingly, preventing prolonged periods of sub-optimal or harmful operation.\n",
       "\n",
       "**Key Limitations:**\n",
       "\n",
       "1.  **Scenario Completeness:** The framework's effectiveness heavily relies on the completeness and relevance of the defined scenarios. Overlooking critical scenarios or underestimating adversarial capabilities can lead to selecting a sub-optimal or vulnerable policy.\n",
       "2.  **Expert Elicitation Burden & Bias:** Defining scenarios and evaluating policy outcomes within them requires significant input from domain experts. This can be time-consuming, costly, and susceptible to expert biases or limited imagination.\n",
       "3.  **Computational Complexity (for large P or S):** If the number of candidate policies ($k$) or scenarios ($m$) is very large, the evaluation step can become computationally intensive, especially if complex simulations are required for each $M(p_i, s_j)$.\n",
       "4.  **Metric Definition Challenge:** Defining a universally agreed-upon and unbiased performance metric $M$ can be challenging, especially when dealing with complex socio-technical systems.\n",
       "5.  **No Guarantee of Optimal Performance:** Minimax regret is a pessimistic strategy; it aims to minimize the *maximum possible loss*, not to achieve the *best possible average outcome*. In environments that turn out to be consistently stable and benign, a more optimistic approach might yield better average performance.\n",
       "\n",
       "**Scenarios Where It May Fail:**\n",
       "\n",
       "1.  **Unforeseeable Black Swans:** If a truly novel and impactful \"black swan\" event occurs that was entirely outside the scope of *any* conceivable scenario (even adversarial ones), the selected policy may be ill-equipped, and the monitoring system might not immediately identify it.\n",
       "2.  **Incapacitated Experts/Lack of Domain Knowledge:** If there are no reliable domain experts available to help define scenarios and assess policy outcomes, or if their knowledge base is fundamentally flawed or outdated, the framework cannot be effectively initialized or maintained.\n",
       "3.  **Complete Observability Breakdown:** If the environment becomes completely unobservable (e.g., all data streams fail, are utterly corrupted, or are entirely under adversarial control), monitoring becomes impossible, and the framework loses its ability to react or adapt.\n",
       "4.  **Sophisticated, Undetectable Adversary:** An adversary with unlimited resources and novel, zero-day attack capabilities that completely bypass all known detection mechanisms and were not envisioned in any adversarial scenario could consistently degrade performance without triggering a response.\n",
       "5.  **Decision Paralysis from Too Many Scenarios:** If the number of scenarios or policies becomes unmanageably large, or if consensus cannot be reached on scenario relevance or performance metrics, the framework could become too slow or complex to implement effectively.\n",
       "\n",
       "---\n",
       "\n",
       "### Competitor 3: **openai/gpt-oss-120b**\n",
       "## 1. Model‑agnostic Decision Rule  \n",
       "**Robust Policy‑Selection (RPS)** – choose the policy whose *worst‑case* expected utility, estimated from the data, is highest.  \n",
       "\n",
       "| Step | Action | Rationale |\n",
       "|------|--------|-----------|\n",
       "| 1️⃣| **Define a finite policy set** 𝒫 = {π₁,…,π_K}. | Keeps the problem tractable and interpretable. |\n",
       "| 2️⃣| **Collect a limited‑size batch** D = { (o_t, a_t, r_t) } where o_t are observable cues, a_t the action taken, r_t the noisy reward. | Works with sparse data; no need for full model. |\n",
       "| 3️⃣| **Bootstrap‑aggregate utility estimates**: for each π_k, draw B ≤ 200 bootstrap resamples of D, compute the average reward ˆU_k^b on each resample, and retain the **α‑lower‑confidence bound** L_k = 𝑄_{α}( {ˆU_k^b} ), e.g. α = 0.05. | Guarantees a pessimistic estimate that protects against over‑fitting and noise. |\n",
       "| 4️⃣| **Apply a simple regularizer**: L_k ← L_k – λ·|π_k| (policy complexity, e.g. number of decision rules). | Favors parsimonious policies → interpretability. |\n",
       "| 5️⃣| **Select** 𝜋* = arg max_{π_k∈𝒫} L_k. | Maximizes the worst‑case plausible performance. |\n",
       "| 6️⃣| **Provide rationale**: report L_k, bootstrap distribution, and complexity penalty for the chosen π*. | Transparent justification for stakeholders. |\n",
       "\n",
       "*Implementation notes* – The algorithm needs only reward samples, no model fitting. Bootstrapping is O(K·B·|D|) and can be run on a single CPU.  \n",
       "\n",
       "---\n",
       "\n",
       "## 2. Assumptions, Data Requirements & Empirical Test Plan  \n",
       "\n",
       "| Assumption | What it means | Data needed | Test (hypothetical) |\n",
       "|------------|---------------|------------|----------------------|\n",
       "| **A1 – Partial observability** | We only see a noisy proxy o_t of the true state. | Observations o_t, actions a_t, rewards r_t. | Simulate a hidden Markov process; compare estimated L_k with ground‑truth utility. |\n",
       "| **A2 – Bounded noise** | Reward noise is sub‑Gaussian with known variance proxy σ². | Repeated reward measurements for same (o,a). | Inject Gaussian noise of variance σ² into synthetic rewards; verify coverage of bootstrap L_k (≈95%). |\n",
       "| **A3 – Limited adversarial contamination** | ≤ ε % of data may be arbitrarily corrupted. | Flagged or suspect records (optional). | Replace ε % of rewards with extreme outliers; check that selected policy remains unchanged if λ is sufficiently large. |\n",
       "| **A4 – Finite policy set captures near‑optimal actions** | True optimal policy is approximated by some π_k ∈ 𝒫. | Policy library constructed a‑priori (e.g., decision trees of depth ≤ 3). | In simulated environments, generate optimal policy and verify it lies within distance ε of a library member. |\n",
       "\n",
       "*Empirical plan*: For each regime (A–C) generate 10 synthetic datasets (|D|=200) and run the RPS pipeline. Record (i) selection stability, (ii) coverage of true utility by L_k, and (iii) false‑positive rate of adversarial detection.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Falsifiable Predictions  \n",
       "\n",
       "| Regime | Prediction (testable) |\n",
       "|--------|-----------------------|\n",
       "| **A – Stable DG with modest noise** | *P1*: The selected policy’s true expected reward will be within 5 % of the oracle optimal reward in ≥ 80 % of simulations. |\n",
       "| **B – Non‑stationarity / concept drift** | *P2*: After a drift point, the bootstrap‑based L_k will drop by > δ (e.g., 0.2) for the previously optimal policy, triggering a policy change in ≤ 5 % of runs (i.e., low false‑alarm). |\n",
       "| **C – Adversarial manipulation** | *P3*: When ≥ ε = 10 % of rewards are replaced by adversarial values, the chosen policy will remain the same as in the clean data case in ≥ 90 % of runs (robustness). |\n",
       "\n",
       "If any prediction is violated, the corresponding assumption (A1‑A4) is falsified and the framework must be revised.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Monitoring & Update Protocol  \n",
       "\n",
       "| Component | Metric / Test | Trigger Condition | Action |\n",
       "|-----------|---------------|-------------------|--------|\n",
       "| **Distributional shift** | Kolmogorov‑Smirnov test on o_t windows (size 50) or ADWIN on reward stream. | p‑value < 0.01 **or** drift magnitude > Δ (e.g., 0.15). | Freeze current policy, recompute bootstrap L_k on the most recent window, select new π*. |\n",
       "| **Data integrity** | % of rewards beyond 3σ of rolling median; checksum anomalies. | > ε_int = 5 % anomalies. | Flag data batch, invoke manual review; if confirmed, discard batch and re‑run RPS on clean data. |\n",
       "| **Bias detection** | Disparity in L_k across protected groups (e.g., Δ_L > 0.1). | Detected bias persists over 2 consecutive windows. | Apply group‑wise regularization λ_g, re‑evaluate; if still biased, invoke escalation. |\n",
       "| **Escalation** | Tier‑1: Automated retraining. Tier‑2: Human‑in‑the‑loop review (policy ethics board). Tier‑3: Full rollback to previously validated policy π_{prev}. |\n",
       "| **Rollback** | If new policy’s L_k < L_{prev} – γ (γ = 0.05) after 30 new samples, revert to π_{prev} and log incident. |\n",
       "\n",
       "All alerts and actions are logged with timestamps for auditability.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Ethical, Legal & Social Assessment  \n",
       "\n",
       "| Issue | Stakeholder impact | Mitigation |\n",
       "|-------|-------------------|------------|\n",
       "| **Fairness** | Unequal utility across demographic groups. | Include group‑wise penalties in λ·|π|, monitor Δ_L, and enforce a fairness threshold (e.g., 80 % parity). |\n",
       "| **Transparency** | Decision makers need to understand why a policy was chosen. | Provide the full bootstrap distribution, complexity penalty, and a concise natural‑language explanation for π*. |\n",
       "| **Privacy** | Observations may contain personal data. | Store only aggregated statistics, apply differential‑privacy noise (ε≈1) to reward counts before bootstrapping. |\n",
       "| **Liability** | Wrong policy could cause harm (e.g., medical triage). | Keep a versioned audit trail; maintain a “safe fallback” policy vetted by regulators. |\n",
       "| **Adversarial exploitation** | Attackers could poison data to force a sub‑optimal policy. | Limit ε‑contamination assumption, monitor integrity metrics, and enforce rate‑limiting on data ingestion. |\n",
       "| **Regulatory compliance** | GDPR, FDA, etc. | Conduct Data Protection Impact Assessment (DPIA), ensure explainability per EU AI Act, and submit the decision rule to the relevant oversight body. |\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Justification, Strengths & Limitations  \n",
       "\n",
       "**Why RPS should work in practice**  \n",
       "* **Robustness** – By basing the decision on a lower‑confidence bound, we protect against sampling noise, limited data, and modest adversarial contamination.  \n",
       "* **Model‑agnosticism** – No need to specify a dynamics model; the method works with any observable‑reward data.  \n",
       "* **Interpretability** – The final rule is a simple policy (e.g., a shallow decision tree) with an explicit utility bound, easy to audit.  \n",
       "* **Computationally cheap** – Bootstrap + simple penalty scales linearly with the number of policies; feasible on edge devices.  \n",
       "\n",
       "**Key limitations**  \n",
       "1. **Policy library dependence** – If the optimal action lies outside 𝒫, performance degrades (violates A4).  \n",
       "2. **Bootstrap reliability under extreme heavy‑tailed noise** – Sub‑Gaussian assumption may fail, leading to overly optimistic L_k.  \n",
       "3. **Detection latency** – Drift tests need a few dozen samples; rapid shifts may cause temporary sub‑optimal actions.  \n",
       "4. **Adversarial budget** – The guarantee holds only up to the assumed ε contamination; a coordinated attack exceeding this budget can break the system.  \n",
       "\n",
       "**Failure scenarios**  \n",
       "* Highly non‑stationary environments where the reward distribution changes faster than the monitoring window.  \n",
       "* When observation space o_t is so severely corrupted that no informative signal remains (complete loss of partial observability).  \n",
       "\n",
       "---\n",
       "\n",
       "## Final Decision Rule (Executable Pseudocode)\n",
       "\n",
       "```python\n",
       "# Robust Policy Selection (RPS)\n",
       "\n",
       "import numpy as np\n",
       "from sklearn.utils import resample\n",
       "\n",
       "def rps(decision_policies, data, B=150, alpha=0.05, lam=0.01):\n",
       "    \"\"\"\n",
       "    decision_policies : list of callables π_k(o) -> a\n",
       "    data             : list of tuples (o, a, r)\n",
       "    B                : number of bootstrap replications\n",
       "    alpha            : lower‑confidence level (e.g., 0.05)\n",
       "    lam              : complexity penalty weight\n",
       "    Returns (π_star, L_star, diagnostics)\n",
       "    \"\"\"\n",
       "    # pre‑compute policy complexities (e.g., number of rules)\n",
       "    complexities = [policy.complexity() for policy in decision_policies]\n",
       "\n",
       "    # extract rewards per policy\n",
       "    rewards_by_policy = {k: [] for k in range(len(decision_policies))}\n",
       "    for (o, a, r) in data:\n",
       "        for k, π in enumerate(decision_policies):\n",
       "            if π(o) == a:                # reward belongs to policy k\n",
       "                rewards_by_policy[k].append(r)\n",
       "\n",
       "    # bootstrap lower bounds\n",
       "    lower_bounds = []\n",
       "    for k, rewards in rewards_by_policy.items():\n",
       "        if len(rewards) == 0:\n",
       "            lower_bounds.append(-np.inf)\n",
       "            continue\n",
       "        boot_means = []\n",
       "        for _ in range(B):\n",
       "            sample = resample(rewards, replace=True, n_samples=len(rewards))\n",
       "            boot_means.append(np.mean(sample))\n",
       "        Lk = np.quantile(boot_means, alpha) - lam * complexities[k]\n",
       "        lower_bounds.append(Lk)\n",
       "\n",
       "    # choose best policy\n",
       "    best_idx = int(np.argmax(lower_bounds))\n",
       "    π_star = decision_policies[best_idx]\n",
       "    L_star = lower_bounds[best_idx]\n",
       "\n",
       "    diagnostics = {\n",
       "        \"lower_bounds\": lower_bounds,\n",
       "        \"complexities\": complexities,\n",
       "        \"bootstrap_means\": boot_means  # last computed (optional)\n",
       "    }\n",
       "    return π_star, L_star, diagnostics\n",
       "```\n",
       "\n",
       "*Deploy*: run `rps` after every monitoring window (≈50 new samples). If the selected `π_star` differs from the current operational policy, trigger the **Update Protocol** (Section 4).  \n",
       "\n",
       "---  \n",
       "\n",
       "*End of answer.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FINAL JUDGING STATE\n",
    "# Combine all answers nicely\n",
    "together = \"\\n\\n\".join(\n",
    "\tf\"### Competitor {i + 1}: **{competitors[i]}**\\n{answer}\"\n",
    "\tfor i, answer in enumerate(answers)\n",
    ")\n",
    "\n",
    "display(Markdown(together))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a4ce72d-464b-4be1-9fcb-7b595b228c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# LLM Competition Judgement"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Judging time: 28.83 seconds**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ranking\n",
       "1. Competitor 1: gpt-5-mini\n",
       "2. Competitor 2: gemini-2.5-flash\n",
       "3. Competitor 3: openai/gpt-oss-120b\n",
       "\n",
       "## Winner Explanation\n",
       "Competitor 1 provides the most accurate and implementable framework by combining robust off-policy evaluation (DR/IPS with clipping), conservative selection (LCB/CVaR), explicit perturbation modeling, clear monitoring/rollback protocols, and thorough ELSI considerations.\n",
       "\n",
       "## Final Winner\n",
       "**Winner: gpt-5-mini**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# — JUDGE PROMPT —\n",
    "start_time = time.time()\n",
    "\n",
    "judge_prompt = f\"\"\"Question asked:\n",
    "{question.strip()}\n",
    "\n",
    "Evaluate these {len(competitors)} responses for accuracy, clarity, reasoning, and overall quality.\n",
    "\n",
    "Responses:\n",
    "{together}\n",
    "\n",
    "Return a clean Markdown response with:\n",
    "1. Ranked list from best to worst\n",
    "2. Short 1-sentence explanation for the winner\n",
    "3. Final winner announcement\n",
    "\n",
    "Use this exact format:\n",
    "## Ranking\n",
    "1. ...\n",
    "2. ...\n",
    "\n",
    "## Winner Explanation\n",
    "...\n",
    "\n",
    "## Final Winner\n",
    "**Winner: ...**\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "\tmodel=\"gpt-5\",\n",
    "\tmessages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    ")\n",
    "\n",
    "judgement = response.choices[0].message.content\n",
    "judge_duration = time.time() - start_time\n",
    "\n",
    "# ── DISPLAY RESULT ──\n",
    "display(Markdown(\"# LLM Competition Judgement\"))\n",
    "display(Markdown(f\"**Judging time: {judge_duration:.2f} seconds**\"))\n",
    "display(Markdown(judgement))\n",
    "\n",
    "# ── SAVE TO FILE WITH DURATION ──\n",
    "with open(\"final_judgement.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tf.write(f\"# LLM Competition Judgement\\n\\n\")\n",
    "\tf.write(f\"**Judged in {judge_duration:.2f} seconds**\\n\\n\")\n",
    "\tf.write(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0e0f6-2774-4a7b-a292-6ebb77cd40ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
